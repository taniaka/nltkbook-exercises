{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK BOOK. Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_affix(word, suffix=True, prefix=False):\n",
    "    if suffix:\n",
    "        hyphen = word.rfind('-')\n",
    "        word = word[:hyphen]\n",
    "    if prefix:\n",
    "        hyphen = word.find('-')\n",
    "        word = word[hyphen+1:]\n",
    "    return word\n",
    "\n",
    "print(remove_affix('dish-es'))\n",
    "print(remove_affix('run-ning'))\n",
    "print(remove_affix('nation-ality'))\n",
    "print(remove_affix('un-do', False, True))\n",
    "print(remove_affix('pre-heat', False, True))\n",
    "print(remove_affix('un-do-ing', prefix=True))\n",
    "print(remove_affix('cat', suffix=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"NLTK\"\n",
    "\n",
    "try:\n",
    "    i = word[-6] # 6th character from the right doesn't exist, so an IndexError is returned.\n",
    "except IndexError:\n",
    "    print(\"Looks like there's an Index error here.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty = 'Monty Python'\n",
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los dos primeros parámetros no están indicados, se reproduce la cadena entera. El tercer parámetro indica cada cuántos caracteres queremos reproducir. Con `1` indicamos que nos interesan todos los caracteres, y como el valor es negativo, la cadena se va a leer desde el final.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+                      uno o más caracteres alpha\n",
    "    [A-Z][a-z]*                    una mayúscula seguida de zero, una o más minúsculas\n",
    "    p[aeiou]{,2}t                  una 'p' seguida de zero, una o dos vocales seguidas de una 't'\n",
    "    \\d+(\\.\\d+)?                    uno o más dígitos seguidos o no de (un punto seguido de uno o más dígitos) \n",
    "    ([^aeiou][aeiou][^aeiou])*     (nada) o (un caracter que no sea una vocal seguido de una vocal seguida de un\n",
    "                                   caracter que no sea una vocal)\n",
    "    \\w+|[^\\w\\s]+                   (uno o más cracteres alfanuméricos) o (uno o más caracteres que no sean  ni\n",
    "                                   alfanuméricos ni de tipo whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "\n",
    "        1. A single determiner (assume that a, an, and the are the only determiners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'\\b([aA][n]?|[tT]he)\\b'\n",
    "re.findall(regexp, 'the tank Anna scandal then an a April The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       2. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'(?:^|\\s)(\\d+(?:\\*\\d+\\+|\\+\\d+\\*)\\d+)(?:\\s|[\\.,!\\?]+ |$)'\n",
    "#  matches expressions that\n",
    "    # contain one '+' and one '*', in any order\n",
    "    # don't contain any '-'\n",
    "    # open the string or are preceded by a whitespace\n",
    "    # close the string or are followed by a whitespace or punctuation mark, followed by a space char.\n",
    "seq = '9+23*45 10*23+45?! 666*2+14,p 9*237+11+*3+2 1+2+4  600*93+5'\n",
    "\n",
    "def find_with_overlap(regexp, seq, capgroup_order=1):\n",
    "    \"\"\" Returns a list of overlapping matches of a regular expression\n",
    "    found in a string.\n",
    "    If the regular expression has several capturable groups, only the match \n",
    "    for one of the groups (the first one, by default) is considered. \n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    while True:\n",
    "        match = re.search(regexp, seq)\n",
    "        if match is None:\n",
    "            break\n",
    "        results.append(match.groups()[capgroup_order-1])\n",
    "        seq = seq[match.end():]\n",
    "    return results\n",
    "\n",
    "print(find_with_overlap(regexp, seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This solution is mostly copied from Ranveer Aggarwal's answer to https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_web(url, decoder='utf8'):\n",
    "    html = request.urlopen(url).read().decode(decoder)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    data = soup.findAll(text=True)\n",
    "    lines = filter(visible, data)\n",
    "    lines = [line.strip().replace('\\n', ' ') for line in lines if line != '\\n']\n",
    "    raw = ' '.join(list(lines))\n",
    "    return raw\n",
    "\n",
    "text_from_web('http://nltk.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus.txt') as f:\n",
    "    raw = ' '.join([line.strip() for line in f.readlines()])\n",
    "\n",
    "pattern = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "          [\\w]+     # get alphanumeric sequences\n",
    "          |\\S       # get punctuation\n",
    "         \n",
    "\"\"\"\n",
    "tokenized = nltk.regexp_tokenize(raw, pattern)\n",
    "tokenized[1500:1550]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = \"\"\"My friend John-Michael Doe from Chicago in the U.S.A. bought a house for $ 2500.8 (i.e. €2012,6)\n",
    "       in Spain on 6/7/2018.\"\"\"\n",
    "\n",
    "pattern2 = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "            \\b((?:\\d{1,2}/){2}(?:\\d{4}|\\d{2}))\\b     # dates, e.g. 5/12/2009\n",
    "          | \\ ((?:[A-Z][\\w\\-\\.]+\\ *)*(?:[A-Z]+[\\w\\-\\.]+)) # proper names, e.g. United Nations or Jean-Paul Sartre \n",
    "          | ([\\$€]\\ ?\\d+(?:[\\.,]\\d+)?)  # monetary amounts, e.g. $12.40\n",
    "\"\"\"\n",
    "\n",
    "tokenized2 = nltk.regexp_tokenize(raw2, pattern2)\n",
    "tokenized2 = [token for tokens in tokenized2 for token in tokens if token]\n",
    "tokenized2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Back in the 90s,\\tI was in a very famous\\n\\n TV show.\"\n",
    "\n",
    "print(\"Splitting with no argument:\")\n",
    "print(sent.split())\n",
    "print(\"Splitting with one space argument:\")\n",
    "print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.split()` sin argumentos siempre separa la cadena en los espacios, las tabulaciones y los saltos de línea. Cualquier secuencia de éstos, siempre dará lugar a una única separación.\n",
    "Si le pasamos un argumento, `.split()` utilizará un algoritmo diferente del que acabamos de describir: separará en cada una de las ocurencias de este argumento. Por ejemplo, si el argumento es un espacio, hará un separación cada vez que encuentre un espacio, y no considerará las tabulaciones y saltos de línea como espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = 'Ziggy played guitar, jamming good with Weird and Gilly and the spiders from Mars.'.split()\n",
    "sorted(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sorted(my_list)` nos devuelve una nueva lista ordenada, mientras que la lista original sigue igual.  \n",
    "\n",
    "`my_list.sort()` ordena cambia la lista original, ordenándolo, pero no la devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:>6}\".format(\"py\"))\n",
    "print(\"{:6}{}\".format(\"py\", \"thon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos el nuevo método de formatear las cadenas, tenemos que sustituir %6s y %-6s por :>6 y :6 respectivamente.   \n",
    "\n",
    "Lo que hacen es imprimir x espacios antes o después de la cadena, respectivamente, donde x corresponde a `6 menos la longitud de la cadena`. En nuestro caso, la longitud de `py` es 2, por lo que se imprimirán 4 espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = (\"what\", \"where\", \"when\", \"why\", \"who\", \"which\", \"whose\", \"whom\")\n",
    "\n",
    "def normalize_token(token):\n",
    "    \"\"\"removes non-alpha cars and lowercases the token.\"\"\"\n",
    "    return re.compile('[^a-zA-Z]').sub('', token).lower()\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "wh_tokens = [token for token in tokens \n",
    "            if normalize_token(token) in wh_words]\n",
    "\n",
    "\n",
    "wh_tokens = sorted(set(wh_tokens), key=lambda s: normalize_token(s))\n",
    "wh_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/2_19.txt') as f:\n",
    "    raw = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "freqs = [[word.split()[0], int(word.split()[1])] for word in raw]\n",
    "    \n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.aemet.es/es/eltiempo/prediccion/municipios/madrid-id28079\"\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "divs = soup.findAll(\"div\", { 'class' : 'no_wrap'})\n",
    "\n",
    "for div in divs:\n",
    "    if div.get_text()[-2:] == '°C': # 1st \"no_wrap\" div with content ending in '°C' corresponds to the current temp.\n",
    "        print (div.get_text())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This solution uses the text_from_url function from exercise 8.\n",
    "\n",
    "def unknown(url):\n",
    "    raw = text_from_web(url)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    words = sorted(set(words))\n",
    "    wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "    unknown_words = [word for word in words if word not in wordlist]\n",
    "    return unknown_words\n",
    "\n",
    "unknown('https://www.ecigarettedirect.co.uk/ashtray-blog/2013/10/interview-inventor-e-cigarette-herbert-a-gilbert.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista de palabras proporcionada por `nltk.corpus.words` siendo bastante incompleta, la mayoría de las palabras \"desconocidas\" son en realidad palabras totalmente válidas y comunes ('credits', 'box', 'looks', etc). Solo en algunos casos se trata de palabras mal escritas ('cigerette'), de trozos de html que BeautifulSoup no ha logrado detectacr y eliminar ('endif'), de nombres propios escritos en minúsculas ('uk') o de partes de contracciones ('didn')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I don't smoke.     ¿¡Don't you know?!\"\n",
    "matches = re.findall(r\"([Dd]o)(n't)|(\\w+)|(\\S)\", sent)\n",
    "\n",
    "matches = [token for match in matches for token in match if token]\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión `«n\\'t|\\w+»` no funciona. Si la aplicamos a \"don't\", tanto \"n't\" como \"don\" son resultados válidos. Sin embargo, los elementos que devuelve `findall` nunca se solapan, entonces solo se devolverá uno de los dos. Después de haberlo probado con más cadenas, he llegado a la conclusión de que se prioriza el elemento más cercano al principio de la cadena. En este caso, es \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "    \n",
    "text = \"\"\"Python is an interpreted,  7 object-oriented, high-level programming language with dynamic semantics. \n",
    "Its high-level built in data structures, combined with dynamic typing and dynamic binding, \n",
    "make it very attractive for Rapid Application Development, as well as for use as a scripting \n",
    "or glue language to connect existing components together. I ate a skinny python.\"\"\"\n",
    "\n",
    "dic = {\n",
    "    r'e': r'3',\n",
    "    r'i': r'1',\n",
    "    r'o': r'0',\n",
    "    r'l': r'|',\n",
    "    r'\\bs': r'$',\n",
    "    r'(?P<start>\\w+)s(?P<end>\\w+)': r'\\g<start>5\\g<end>',\n",
    "    r'\\.': r'5w33t!',\n",
    "    r'ate': r'8',\n",
    "    r'd': r'stu',\n",
    "    r'y': r'zz'\n",
    "}\n",
    "\n",
    "multiple_replace(text.lower(), dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "    Write a function to convert a word to Pig Latin.\n",
    "    Write code that converts text, instead of individual words.\n",
    "    Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_pig(word):\n",
    "    if not word.isalpha():\n",
    "        return word\n",
    "    pattern = re.compile(r'(?P<start>^([yY])?[^aeioAEIO(qu)(Qu)(QU)(qU)uyUY]*([qQ][uU])?)(?P<end>\\w*)')\n",
    "    pig_word = re.sub(pattern, \n",
    "                      r'\\g<end>\\g<start>ay', \n",
    "                      word)\n",
    "    if word.istitle():\n",
    "        pig_word = pig_word[0].title() + pig_word[1:]\n",
    "    return pig_word\n",
    "\n",
    "def tokenize_text(raw):\n",
    "    tokens= re.findall(r\"[a-zA-Z]+|\\S|\\s\", raw)\n",
    "    return tokens\n",
    "\n",
    "def sent_to_pig(sent):\n",
    "    tokens = tokenize_text(sent)\n",
    "    pig_tokens = [word_to_pig(token)  \n",
    "                  for token in tokens]\n",
    "    pig_sent = ''.join(pig_tokens)\n",
    "    return pig_sent\n",
    "\n",
    "\n",
    "words = ['look', 'stamp', 'GRound', 'Cherry', 'idle', 'Quick', \n",
    "         'squeeze', 'yellow', 'style', 'You', 'SYD']\n",
    "sent = \"\"\"This is a quite long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\"\"\"\n",
    "\n",
    "print('Converting words to Pig Latin:')\n",
    "for word in words:\n",
    "    print('{} --> {}'.format(word, word_to_pig(word)))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Converting sentence into Pig Latin:\")\n",
    "print(sent)\n",
    "print('------->')\n",
    "print(sent_to_pig(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ' '.join(nltk.corpus.udhr.words('Hungarian_Magyar-Latin1'))\n",
    "vowel_seqs = set(re.findall(r'[aeiouAEIOUáéíóöőúüűÁÉÍÓÖŐÚÜŰ]{2,}', raw))\n",
    "vowel_seqs = [tuple(seq) for seq in vowel_seqs]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(vowel_seqs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
