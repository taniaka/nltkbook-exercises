{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK BOOK. Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "do\n",
      "heat\n",
      "do\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "def remove_affix(word, suffix=True, prefix=False):\n",
    "    if suffix:\n",
    "        hyphen = word.rfind('-')\n",
    "        word = word[:hyphen]\n",
    "    if prefix:\n",
    "        hyphen = word.find('-')\n",
    "        word = word[hyphen+1:]\n",
    "    return word\n",
    "\n",
    "print(remove_affix('dish-es'))\n",
    "print(remove_affix('run-ning'))\n",
    "print(remove_affix('nation-ality'))\n",
    "print(remove_affix('un-do', False, True))\n",
    "print(remove_affix('pre-heat', False, True))\n",
    "print(remove_affix('un-do-ing', prefix=True))\n",
    "print(remove_affix('cat', suffix=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like there's an Index error here.\n"
     ]
    }
   ],
   "source": [
    "word = \"NLTK\"\n",
    "\n",
    "try:\n",
    "    i = word[-6] # 6th character from the right doesn't exist, so an IndexError is returned.\n",
    "except IndexError:\n",
    "    print(\"Looks like there's an Index error here.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los dos primeros parámetros no están indicados, se reproduce la cadena entera. El tercer parámetro indica cada cuántos caracteres queremos reproducir. Con `1` indicamos que nos interesan todos los caracteres, y como el valor es negativo, la cadena se va a leer desde el final.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+                      uno o más caracteres alpha\n",
    "    [A-Z][a-z]*                    una mayúscula seguida de zero, una o más minúsculas\n",
    "    p[aeiou]{,2}t                  una 'p' seguida de zero, una o dos vocales seguidas de una 't'\n",
    "    \\d+(\\.\\d+)?                    uno o más dígitos seguidos o no de (un punto seguido de uno o más dígitos) \n",
    "    ([^aeiou][aeiou][^aeiou])*     (nada) o (un caracter que no sea una vocal seguido de una vocal seguida de un\n",
    "                                   caracter que no sea una vocal)\n",
    "    \\w+|[^\\w\\s]+                   (uno o más cracteres alfanuméricos) o (uno o más caracteres que no sean  ni\n",
    "                                   alfanuméricos ni de tipo whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "\n",
    "        1. A single determiner (assume that a, an, and the are the only determiners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'an', 'a', 'The']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = r'\\b([aA][n]?|[tT]he)\\b'\n",
    "re.findall(regexp, 'the tank Anna scandal then an a April The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       2. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9+23*45', '10*23+45', '600*93+5']\n"
     ]
    }
   ],
   "source": [
    "regexp = r'(?:^|\\s)(\\d+(?:\\*\\d+\\+|\\+\\d+\\*)\\d+)(?:\\s|[\\.,!\\?]+ |$)'\n",
    "#  matches expressions that\n",
    "    # contain one '+' and one '*', in any order\n",
    "    # don't contain any '-'\n",
    "    # open the string or are preceded by a whitespace\n",
    "    # close the string or are followed by a whitespace or punctuation mark, followed by a space char.\n",
    "seq = '9+23*45 10*23+45?! 666*2+14,p 9*237+11+*3+2 1+2+4  600*93+5'\n",
    "\n",
    "def find_with_overlap(regexp, seq, capgroup_order=1):\n",
    "    \"\"\" Returns a list of overlapping matches of a regular expression\n",
    "    found in a string.\n",
    "    If the regular expression has several capturable groups, only the match \n",
    "    for one of the groups (the first one, by default) is considered. \n",
    "    \"\"\"\n",
    "    results=[]\n",
    "    while True:\n",
    "        match = re.search(regexp, seq)\n",
    "        if match is None:\n",
    "            break\n",
    "        results.append(match.groups()[capgroup_order-1])\n",
    "        seq = seq[match.end():]\n",
    "    return results\n",
    "\n",
    "print(find_with_overlap(regexp, seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK 3.2.5 documentation next | modules | index Natural Language Toolkit ¶ NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum . Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.” Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The book is being updated for Python 3 and NLTK 3. (The original Python 2 version is still available at http://nltk.org/book_1ed .) Some simple things you can do with NLTK ¶ Tokenize and tag some text: >>> import  nltk >>> sentence  =  \"\"\"At eight o\\'clock on Thursday morning ... Arthur didn\\'t feel very good.\"\"\" >>> tokens  =  nltk . word_tokenize ( sentence ) >>> tokens [\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\', \\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\'] >>> tagged  =  nltk . pos_tag ( tokens ) >>> tagged [ 0 : 6 ] [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')] Identify named entities: >>> entities  =  nltk . chunk . ne_chunk ( tagged ) >>> entities Tree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'), Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]), (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'), (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')]) Display a parse tree: >>> from  nltk.corpus  import  treebank >>> t  =  treebank . parsed_sents ( \\'wsj_0001.mrg\\' )[ 0 ] >>> t . draw () NB. If you publish work that uses NLTK, please cite the NLTK book as follows: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python .  O’Reilly Media Inc. Next Steps ¶ sign up for release announcements join in the discussion Contents ¶ NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Index Module Index Search Page Table Of Contents NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Search next | modules | index Show Source © Copyright 2017, NLTK Project.       Last updated on Sep 24, 2017.       Created using Sphinx 1.5.5.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution is mostly copied from Ranveer Aggarwal's answer to https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_web(url, decoder='utf8'):\n",
    "    html = request.urlopen(url).read().decode(decoder)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    data = soup.findAll(text=True)\n",
    "    lines = filter(visible, data)\n",
    "    lines = [line.strip().replace('\\n', ' ') for line in lines if line != '\\n']\n",
    "    raw = ' '.join(list(lines))\n",
    "    return raw\n",
    "\n",
    "text_from_web('http://nltk.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bufo', 'o', 'grotesco', 'y', 'lo', 'trágico', 'estén', 'mezclados', 'o', 'yuxtapuestos', ',', 'sino', 'fundidos', 'y', 'confundidos', 'en', 'uno', '.', 'Y', 'como', 'yo', 'le', 'hiciese', 'observar', 'que', 'eso', 'no', 'es', 'sino', 'el', 'más', 'desenfrenado', 'romanticismo', ',', 'me', 'contestó', ':', '«', 'no', 'lo', 'niego', ',', 'pero', 'con', 'poner', 'motes', 'a', 'las', 'cosas', 'no']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/corpus.txt') as f:\n",
    "    raw = ' '.join([line.strip() for line in f.readlines()])\n",
    "\n",
    "pattern = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "          [\\w]+     # get alphanumeric sequences\n",
    "          |\\S       # get punctuation\n",
    "         \n",
    "\"\"\"\n",
    "tokenized = nltk.regexp_tokenize(raw, pattern)\n",
    "tokenized[1500:1550]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John-Michael Doe', 'Chicago', 'U.S.A.', '$ 2500.8', '€2012,6', 'Spain', '6/7/2018']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2 = \"\"\"My friend John-Michael Doe from Chicago in the U.S.A. bought a house for $ 2500.8 (i.e. €2012,6)\n",
    "       in Spain on 6/7/2018.\"\"\"\n",
    "\n",
    "pattern2 = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "            \\b((?:\\d{1,2}/){2}(?:\\d{4}|\\d{2}))\\b     # dates, e.g. 5/12/2009\n",
    "          | \\ ((?:[A-Z][\\w\\-\\.]+\\ *)*(?:[A-Z]+[\\w\\-\\.]+)) # proper names, e.g. United Nations or Jean-Paul Sartre \n",
    "          | ([\\$€]\\ ?\\d+(?:[\\.,]\\d+)?)  # monetary amounts, e.g. $12.40\n",
    "\"\"\"\n",
    "\n",
    "tokenized2 = nltk.regexp_tokenize(raw2, pattern2)\n",
    "tokenized2 = [token for tokens in tokenized2 for token in tokens if token]\n",
    "tokenized2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting with no argument:\n",
      "['Back', 'in', 'the', '90s,', 'I', 'was', 'in', 'a', 'very', 'famous', 'TV', 'show.']\n",
      "Splitting with one space argument:\n",
      "['Back', 'in', 'the', '90s,\\tI', 'was', 'in', 'a', 'very', 'famous\\n\\n', 'TV', 'show.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Back in the 90s,\\tI was in a very famous\\n\\n TV show.\"\n",
    "\n",
    "print(\"Splitting with no argument:\")\n",
    "print(sent.split())\n",
    "print(\"Splitting with one space argument:\")\n",
    "print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.split()` sin argumentos siempre separa la cadena en los espacios, las tabulaciones y los saltos de línea. Cualquier secuencia de éstos, siempre dará lugar a una única separación.\n",
    "Si le pasamos un argumento, `.split()` utilizará un algoritmo diferente del que acabamos de describir: separará en cada una de las ocurencias de este argumento. Por ejemplo, si el argumento es un espacio, hará un separación cada vez que encuentre un espacio, y no considerará las tabulaciones y saltos de línea como espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = 'Ziggy played guitar, jamming good with Weird and Gilly and the spiders from Mars.'.split()\n",
    "sorted(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ziggy', 'played', 'guitar,', 'jamming', 'good', 'with', 'Weird', 'and', 'Gilly', 'and', 'the', 'spiders', 'from', 'Mars.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sorted(my_list)` nos devuelve una nueva lista ordenada, mientras que la lista original sigue igual.  \n",
    "\n",
    "`my_list.sort()` ordena cambia la lista original, ordenándolo, pero no la devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    py\n",
      "py    thon\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>6}\".format(\"py\"))\n",
    "print(\"{:6}{}\".format(\"py\", \"thon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos el nuevo método de formatear las cadenas, tenemos que sustituir %6s y %-6s por :>6 y :6 respectivamente.   \n",
    "\n",
    "Lo que hacen es imprimir x espacios antes o después de la cadena, respectivamente, donde x corresponde a `6 menos la longitud de la cadena`. En nuestro caso, la longitud de `py` es 2, por lo que se imprimirán 4 espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'what', 'When', 'when', 'Where', 'where', 'which', 'Which', 'who', 'Who', 'WHO', 'whom', 'whose', 'why', 'Why', \"'Why\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_words = (\"what\", \"where\", \"when\", \"why\", \"who\", \"which\", \"whose\", \"whom\")\n",
    "\n",
    "def normalize_token(token):\n",
    "    \"\"\"removes non-alpha cars and lowercases the token.\"\"\"\n",
    "    return re.compile('[^a-zA-Z]').sub('', token).lower()\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "wh_tokens = [token for token in tokens \n",
    "            if normalize_token(token) in wh_words]\n",
    "\n",
    "\n",
    "wh_tokens = sorted(set(wh_tokens), key=lambda s: normalize_token(s))\n",
    "wh_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['random', 10], ['words', 44], ['collection', 90], ['for', 75], ['the', 89], ['exercise', 36], ['number', 19], ['nineteen', 34], ['from', 6], ['chapter', 2], ['two', 906]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/2_19.txt') as f:\n",
    "    raw = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "freqs = [[word.split()[0], int(word.split()[1])] for word in raw]\n",
    "    \n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8°C\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.aemet.es/es/eltiempo/prediccion/municipios/madrid-id28079\"\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "divs = soup.findAll(\"div\", { 'class' : 'no_wrap'})\n",
    "\n",
    "for div in divs:\n",
    "    if div.get_text()[-2:] == '°C': # 1st \"no_wrap\" div with content ending in '°C' corresponds to the current temp.\n",
    "        print (div.get_text())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accessable', 'admitting', 'ages', 'albert', 'alternatives', 'angles', 'approached', 'articles', 'ashtray', 'asked', 'asks', 'assembled', 'awited', 'began', 'benefited', 'benefits', 'blog', 'blogger', 'bohr', 'bookmarked', 'box', 'boyfriend', 'brands', 'brewed', 'carefull', 'causes', 'christine', 'cigarettes', 'cigerette', 'clicked', 'co', 'com', 'commenting', 'comments', 'companies', 'complexions', 'concluded', 'controlled', 'cooked', 'cookies', 'corners', 'coughing', 'coworkers', 'credits', 'deaths', 'delivered', 'dictates', 'didn', 'directions', 'disposables', 'doesn', 'domingue', 'draining', 'drawings', 'ecigarettedirect', 'edges', 'email', 'emblazoned', 'embraced', 'endif', 'enthousiastic', 'etc', 'events', 'exhumed', 'extatic', 'eyes', 'feet', 'fields', 'fights', 'filed', 'flavors', 'flavoured', 'flitted', 'follows', 'followup', 'friends', 'gic', 'governments', 'groceries', 'grows', 'guides', 'hands', 'happened', 'happens', 'has', 'hast', 'healthier', 'heated', 'helped', 'html', 'https', 'humongously', 'hundreds', 'identified', 'implements', 'improved', 'including', 'infections', 'inhaling', 'introduced', 'invented', 'inventing', 'inventions', 'invited', 'items', 'joined', 'kidding', 'kuemper', 'legislators', 'lighted', 'lives', 'll', 'looks', 'loved', 'lungs', 'makes', 'markets', 'medicated', 'medications', 'members', 'menthols', 'mentioned', 'michael', 'months', 'moves', 'mr', 'needed', 'nestled', 'notes', 'offered', 'offers', 'oozing', 'organisations', 'others', 'packaging', 'packing', 'packs', 'paid', 'parents', 'passed', 'paul', 'photos', 'pictures', 'pillows', 'pioneered', 'placed', 'places', 'pm', 'posts', 'powers', 'predicted', 'proceeded', 'produces', 'producing', 'products', 'promoting', 'protected', 'prototypes', 'proud', 'provides', 'published', 'purchasing', 'pursuing', 'quacks', 'reached', 'reared', 'recommendations', 'recruited', 'referring', 'regards', 'relatives', 'reported', 'revamped', 'rewards', 'rights', 'ripped', 'roadmap', 'roamed', 'saviour', 'says', 'screamed', 'searched', 'shakedly', 'shared', 'shouted', 'showed', 'smokers', 'solved', 'staples', 'started', 'starts', 'stored', 'styrofoam', 'supplies', 'swims', 'taxes', 'teenager', 'tells', 'texting', 'things', 'thoughts', 'tis', 'tutorials', 'types', 'uk', 'unforgivig', 'updated', 'updating', 'using', 'vape', 'vaper', 'vapers', 'vaping', 'vapour', 've', 'ventured', 'versions', 'viewing', 'views', 'voices', 'walks', 'wanted', 'wants', 'warnings', 'wasn', 'words', 'www', 'years', 'yielded']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution uses the text_from_url function from exercise 8.\n",
    "\n",
    "def unknown(url):\n",
    "    raw = text_from_web(url)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    words = sorted(set(words))\n",
    "    wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "    unknown_words = [word for word in words if word not in wordlist]\n",
    "    return unknown_words\n",
    "\n",
    "unknown('https://www.ecigarettedirect.co.uk/ashtray-blog/2013/10/interview-inventor-e-cigarette-herbert-a-gilbert.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista de palabras proporcionada por `nltk.corpus.words` siendo bastante incompleta, la mayoría de las palabras \"desconocidas\" son en realidad palabras totalmente válidas y comunes ('credits', 'box', 'looks', etc). Solo en algunos casos se trata de palabras mal escritas ('cigerette'), de trozos de html que BeautifulSoup no ha logrado detectacr y eliminar ('endif'), de nombres propios escritos en minúsculas ('uk') o de partes de contracciones ('didn')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'smoke', '.', '¿', '¡', 'Do', \"n't\", 'you', 'know', '?', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I don't smoke.     ¿¡Don't you know?!\"\n",
    "matches = re.findall(r\"([Dd]o)(n't)|(\\w+)|(\\S)\", sent)\n",
    "\n",
    "matches = [token for match in matches for token in match if token]\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión `«n\\'t|\\w+»` no funciona. Si la aplicamos a \"don't\", tanto \"n't\" como \"don\" son resultados válidos. Sin embargo, los elementos que devuelve `findall` nunca se solapan, entonces solo se devolverá uno de los dos. Después de haberlo probado con más cadenas, he llegado a la conclusión de que se prioriza el elemento más cercano al principio de la cadena. En este caso, es \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pzzth0n 1s an 1nt3rpr3t3stu,  7 0bj3ct-0r13nt3stu, h1gh-|3v3| pr0gramm1ng |anguag3 w1th stuzznam1c $3mant1cs5w33t! \\n1ts h1gh-|3v3| bu1|t 1n stuata $tructur3s, c0mb1n3stu w1th stuzznam1c tzzp1ng anstu stuzznam1c b1nstu1ng, \\nmak3 1t v3rzz attract1v3 f0r rap1stu app|1cat10n stu3v3|0pm3nt, as w3|| as f0r u53 as a $cr1pt1ng \\n0r g|u3 |anguag3 t0 c0nn3ct 3x15t1ng c0mp0n3nts t0g3th3r5w33t! 1 at3 a $k1nnzz pzzth0n5w33t!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "    \n",
    "text = \"\"\"Python is an interpreted,  7 object-oriented, high-level programming language with dynamic semantics. \n",
    "Its high-level built in data structures, combined with dynamic typing and dynamic binding, \n",
    "make it very attractive for Rapid Application Development, as well as for use as a scripting \n",
    "or glue language to connect existing components together. I ate a skinny python.\"\"\"\n",
    "\n",
    "dic = {\n",
    "    r'e': r'3',\n",
    "    r'i': r'1',\n",
    "    r'o': r'0',\n",
    "    r'l': r'|',\n",
    "    r'\\bs': r'$',\n",
    "    r'(?P<start>\\w+)s(?P<end>\\w+)': r'\\g<start>5\\g<end>',\n",
    "    r'\\.': r'5w33t!',\n",
    "    r'ate': r'8',\n",
    "    r'd': r'stu',\n",
    "    r'y': r'zz'\n",
    "}\n",
    "\n",
    "multiple_replace(text.lower(), dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "    Write a function to convert a word to Pig Latin.\n",
    "    Write code that converts text, instead of individual words.\n",
    "    Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting words to Pig Latin:\n",
      "look --> ooklay\n",
      "stamp --> ampstay\n",
      "GRound --> oundGRay\n",
      "Cherry --> ErryChay\n",
      "idle --> idleay\n",
      "Quick --> IckQuay\n",
      "squeeze --> eezesquay\n",
      "yellow --> ellowyay\n",
      "style --> ylestay\n",
      "You --> OuYay\n",
      "SYD --> YDSay\n",
      "\n",
      "\n",
      "Converting sentence into Pig Latin:\n",
      "This is a quite long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\n",
      "------->\n",
      "IsThay isay aay itequay onglay \"entencesay\" 5+2... EsYay, itay isay!!! ¿Arenay'tay erethay otway entencessay erehay?\n"
     ]
    }
   ],
   "source": [
    "def word_to_pig(word):\n",
    "    if not word.isalpha():\n",
    "        return word\n",
    "    pattern = re.compile(r'(?P<start>^([yY])?[^aeioAEIO(qu)(Qu)(QU)(qU)uyUY]*([qQ][uU])?)(?P<end>\\w*)')\n",
    "    pig_word = re.sub(pattern, \n",
    "                      r'\\g<end>\\g<start>ay', \n",
    "                      word)\n",
    "    if word.istitle():\n",
    "        pig_word = pig_word[0].title() + pig_word[1:]\n",
    "    return pig_word\n",
    "\n",
    "def tokenize_text(raw):\n",
    "    tokens= re.findall(r\"[a-zA-Z]+|\\S|\\s\", raw)\n",
    "    return tokens\n",
    "\n",
    "def sent_to_pig(sent):\n",
    "    tokens = tokenize_text(sent)\n",
    "    pig_tokens = [word_to_pig(token)  \n",
    "                  for token in tokens]\n",
    "    pig_sent = ''.join(pig_tokens)\n",
    "    return pig_sent\n",
    "\n",
    "\n",
    "words = ['look', 'stamp', 'GRound', 'Cherry', 'idle', 'Quick', \n",
    "         'squeeze', 'yellow', 'style', 'You', 'SYD']\n",
    "sent = \"\"\"This is a quite long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\"\"\"\n",
    "\n",
    "print('Converting words to Pig Latin:')\n",
    "for word in words:\n",
    "    print('{} --> {}'.format(word, word_to_pig(word)))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Converting sentence into Pig Latin:\")\n",
    "print(sent)\n",
    "print('------->')\n",
    "print(sent_to_pig(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a e i u á é \n",
      "a 0 0 1 1 0 0 \n",
      "e 1 1 1 0 0 1 \n",
      "i 1 1 1 0 1 0 \n",
      "á 0 0 0 0 0 1 \n",
      "ó 1 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "raw = ' '.join(nltk.corpus.udhr.words('Hungarian_Magyar-Latin1'))\n",
    "vowel_seqs = set(re.findall(r'[aeiouAEIOUáéíóöőúüűÁÉÍÓÖŐÚÜŰ]{2,}', raw))\n",
    "vowel_seqs = [tuple(seq) for seq in vowel_seqs]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(vowel_seqs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
