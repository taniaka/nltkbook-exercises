{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK BOOK. Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "do\n",
      "heat\n",
      "do\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "def remove_affix(word, suffix=True, prefix=False):\n",
    "    if suffix:\n",
    "        hyphen = word.rfind('-')\n",
    "        word = word[:hyphen]\n",
    "    if prefix:\n",
    "        hyphen = word.find('-')\n",
    "        word = word[hyphen+1:]\n",
    "    return word\n",
    "\n",
    "print(remove_affix('dish-es'))\n",
    "print(remove_affix('run-ning'))\n",
    "print(remove_affix('nation-ality'))\n",
    "print(remove_affix('un-do', False, True))\n",
    "print(remove_affix('pre-heat', False, True))\n",
    "print(remove_affix('un-do-ing', prefix=True))\n",
    "print(remove_affix('cat', suffix=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like there's an Index error here.\n"
     ]
    }
   ],
   "source": [
    "word = \"NLTK\"\n",
    "\n",
    "try:\n",
    "    i = word[-6] # 6th character from the right doesn't exist, so an IndexError is returned.\n",
    "except IndexError:\n",
    "    print(\"Looks like there's an Index error here.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los dos primeros parámetros no están indicados, se reproduce la cadena entera. El tercer parámetro indica cada cuántos caracteres queremos reproducir. Con `1` indicamos que nos interesan todos los caracteres, y como el valor es negativo, la cadena se va a leer desde el final.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+                      uno o más caracteres alpha\n",
    "    [A-Z][a-z]*                    una mayúscula seguida de zero, una o más minúsculas\n",
    "    p[aeiou]{,2}t                  una 'p' seguida de zero, una o dos vocales seguida(s) de una 't'\n",
    "    \\d+(\\.\\d+)?                    uno o más dígitos seguidos o no de (un punto seguido de uno o más dígitos) \n",
    "    ([^aeiou][aeiou][^aeiou])*     (nada) o una o más secuencias de (un caracter que no sea una vocal seguido de una    \n",
    "                                   vocal seguida de uncaracter que no sea una vocal)\n",
    "    \\w+|[^\\w\\s]+                   (uno o más caracteres alfanuméricos) o (uno o más caracteres que no sean  ni\n",
    "                                   alfanuméricos ni de tipo whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "\n",
    "        1. A single determiner (assume that a, an, and the are the only determiners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'an', 'a', 'The']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = r'\\b([aA][n]?|[tT]he)\\b'\n",
    "re.findall(regexp, 'the tank Anna scandal then an a April The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       2. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9+23*45', '10*23+45', '666*2+14', '600*93+5', '700+7*2']\n"
     ]
    }
   ],
   "source": [
    "# re doesn't support overlapping, but regex does\n",
    "\n",
    "import regex\n",
    "\n",
    "pattern = regex.compile(r'(?:^|\\s)(\\d+(?:\\*\\d+\\+|\\+\\d+\\*)\\d+)(?:\\s|[\\.,!\\?]+|$)')\n",
    "#  matches expressions that\n",
    "    # contain one '+' and one '*', in any order\n",
    "    # open the string or are preceded by a whitespace\n",
    "    # close the string or are followed by a whitespace or punctuation mark.\n",
    "    \n",
    "seq = '9+23*45 10*23+45?! 666*2+14,p 9*237+11+*3+2 1+2+4  600*93+5 700+7*2.'\n",
    "\n",
    "matches = regex.findall(pattern, seq, overlapped=True)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK 3.2.5 documentation next | modules | index Natural Language Toolkit ¶ NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum . Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.” Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The book is being updated for Python 3 and NLTK 3. (The original Python 2 version is still available at http://nltk.org/book_1ed .) Some simple things you can do with NLTK ¶ Tokenize and tag some text: >>> import  nltk >>> sentence  =  \"\"\"At eight o\\'clock on Thursday morning ... Arthur didn\\'t feel very good.\"\"\" >>> tokens  =  nltk . word_tokenize ( sentence ) >>> tokens [\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\', \\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\'] >>> tagged  =  nltk . pos_tag ( tokens ) >>> tagged [ 0 : 6 ] [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')] Identify named entities: >>> entities  =  nltk . chunk . ne_chunk ( tagged ) >>> entities Tree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'), Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]), (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'), (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')]) Display a parse tree: >>> from  nltk.corpus  import  treebank >>> t  =  treebank . parsed_sents ( \\'wsj_0001.mrg\\' )[ 0 ] >>> t . draw () NB. If you publish work that uses NLTK, please cite the NLTK book as follows: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python .  O’Reilly Media Inc. Next Steps ¶ sign up for release announcements join in the discussion Contents ¶ NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Index Module Index Search Page Table Of Contents NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Search next | modules | index Show Source © Copyright 2017, NLTK Project.       Last updated on Sep 24, 2017.       Created using Sphinx 1.5.5.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution is mostly copied from Ranveer Aggarwal's answer to https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_web(url, decoder='utf8'):\n",
    "    html = request.urlopen(url).read().decode(decoder)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    data = soup.findAll(text=True)\n",
    "    lines = filter(visible, data)\n",
    "    lines = [line.strip().replace('\\n', ' ') for line in lines if line != '\\n']\n",
    "    raw = ' '.join(list(lines))\n",
    "    return raw\n",
    "\n",
    "text_from_web('http://nltk.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', ']', ',', ',', '.', '—', ',', '.', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.', '.', ',', ',', '.', ',', '(', '.', ')', '—', '—', ',', ',', ',', ',', ',', '.', ',', '.', ',', '.', '.', ',']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(filepath):\n",
    "    with open(filepath) as f:\n",
    "        raw = ' '.join([line.strip() for line in f.readlines()])\n",
    "    return raw\n",
    "\n",
    "pattern = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "          [^\\s\\w]   # get punctuation         \n",
    "\"\"\"\n",
    "\n",
    "raw = load('../data/corpus.txt')\n",
    "tokenized = nltk.regexp_tokenize(raw, pattern)\n",
    "tokenized[:50] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John-Michael Doe', 'Chicago', 'U.S.A.', '$ 2500.8', '€2012,6', 'Spain', '6/7/2018']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2 = \"\"\"My friend John-Michael Doe from Chicago in the U.S.A. bought a house for $ 2500.8 (i.e. €2012,6)\n",
    "       in Spain on 6/7/2018.\"\"\"\n",
    "\n",
    "pattern2 = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "            \\b((?:\\d{1,2}/){2}(?:\\d{4}|\\d{2}))\\b     # dates, e.g. 5/12/2009\n",
    "          | \\ ((?:[A-Z][\\w\\-\\.]+\\ *)*(?:[A-Z]+[\\w\\-\\.]+)) # proper names, e.g. United Nations or Jean-Paul Sartre,\n",
    "          # when using the verbose flag, we can no longer use ' ' to match a space character, hence the use of '\\ '\n",
    "          | ([\\$€]\\ ?\\d+(?:[\\.,]\\d+)?)  # monetary amounts, e.g. $12.40\n",
    "\"\"\"\n",
    "\n",
    "tokenized2 = nltk.regexp_tokenize(raw2, pattern2)\n",
    "tokenized2 = [token for tokens in tokenized2 for token in tokens if token]\n",
    "tokenized2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting with no argument:\n",
      "['Back', 'in', 'the', '90s,', 'I', 'was', 'in', 'a', 'very', 'famous', 'TV', 'show.']\n",
      "Splitting with one space argument:\n",
      "['Back', 'in', 'the', '90s,\\tI', 'was', 'in', 'a', 'very', 'famous\\n\\n', 'TV', 'show.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Back in the 90s,\\tI was in a very famous\\n\\n TV show.\"\n",
    "\n",
    "print(\"Splitting with no argument:\")\n",
    "print(sent.split())\n",
    "print(\"Splitting with one space argument:\")\n",
    "print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.split()` sin argumentos siempre separa la cadena en los espacios, las tabulaciones y los saltos de línea. Cualquier secuencia de éstos, siempre dará lugar a una única separación.\n",
    "Si le pasamos un argumento, `.split()` utilizará un algoritmo diferente: separará en cada una de las ocurencias de este argumento. Por ejemplo, si el argumento es un espacio, hará un separación cada vez que encuentre un espacio, y no considerará las tabulaciones y saltos de línea como espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'Ziggy played guitar, jamming good with Weird and Gilly and the spiders from Mars.'.split()\n",
    "sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ziggy', 'played', 'guitar,', 'jamming', 'good', 'with', 'Weird', 'and', 'Gilly', 'and', 'the', 'spiders', 'from', 'Mars.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sorted(my_list)` nos devuelve una nueva lista ordenada, mientras que la lista original sigue igual.  \n",
    "\n",
    "`my_list.sort()` cambia la lista original, ordenándola, pero no la devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    py\n",
      "py    thon\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>6}\".format(\"py\"))\n",
    "print(\"{:6}{}\".format(\"py\", \"thon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos el nuevo método de formatear las cadenas, tenemos que sustituir %6s y %-6s por :>6 y :6 respectivamente.   \n",
    "\n",
    "Lo que hacen es imprimir x espacios antes o después de la cadena, respectivamente, donde x corresponde a `6 menos la longitud de la cadena`. En nuestro caso, la longitud de `py` es 2, por lo que se imprimirán 4 espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Why\", 'What', 'what', 'when', 'When', 'Where', 'where', 'which', 'Which', 'WHO', 'who', 'Who', 'whom', 'whose', 'Why', 'why']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_token(token):\n",
    "    \"\"\"lowercases the token and removes non-alpha chars from it.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z]', '', token).lower()\n",
    "\n",
    "def get_wh_tokens(tokens):\n",
    "    \"\"\"returns a list of all the wh_words contained in tokens, \n",
    "    preserving their original spelling and including non-alpha chars they may contain\"\"\"\n",
    "    wh_words = (\"what\", \"where\", \"when\", \"why\", \"who\", \"which\", \"whose\", \"whom\")\n",
    "    wh_tokens = [token for token in tokens \n",
    "                if normalize_token(token) in wh_words]\n",
    "    wh_tokens = sorted(set(wh_tokens), key=lambda s: s.lower())\n",
    "    return wh_tokens\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "wh_tokens = get_wh_tokens(tokens)\n",
    "\n",
    "wh_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['random', 10], ['words', 44], ['collection', 90], ['for', 75], ['the', 89], ['exercise', 36], ['number', 19], ['nineteen', 34], ['from', 6], ['chapter', 2], ['two', 906]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/2_19.txt') as f:\n",
    "    raw = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "freqs = [[word.split()[0], int(word.split()[1])] for word in raw]\n",
    "    \n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11°C\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.aemet.es/es/eltiempo/prediccion/municipios/madrid-id28079\"\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "divs = soup.findAll(\"div\", { 'class' : 'no_wrap'})\n",
    "\n",
    "for div in divs:\n",
    "    if div.get_text()[-2:] == '°C': # 1st \"no_wrap\" div with content ending in '°C' corresponds to the current temp.\n",
    "        print (div.get_text())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accessable', 'admitting', 'ages', 'albert', 'alternatives', 'angles', 'approached', 'articles', 'ashtray', 'asked', 'asks', 'assembled', 'awited', 'began', 'benefited', 'benefits', 'blog', 'blogger', 'bohr', 'bookmarked', 'box', 'boyfriend', 'brands', 'brewed', 'carefull', 'causes', 'christine', 'cigarettes', 'cigerette', 'clicked', 'co', 'com', 'commenting', 'comments', 'companies', 'complexions', 'concluded', 'controlled', 'cooked', 'cookies', 'corners', 'coughing', 'coworkers', 'credits', 'deaths', 'delivered', 'dictates', 'didn', 'directions', 'disposables', 'doesn', 'domingue', 'draining', 'drawings', 'ecigarettedirect', 'edges', 'email', 'emblazoned', 'embraced', 'endif', 'enthousiastic', 'etc', 'events', 'exhumed', 'extatic', 'eyes', 'feet', 'fields', 'fights', 'filed', 'flavors', 'flavoured', 'flitted', 'follows', 'followup', 'friends', 'gic', 'governments', 'groceries', 'grows', 'guides', 'hands', 'happened', 'happens', 'has', 'hast', 'healthier', 'heated', 'helped', 'html', 'https', 'humongously', 'hundreds', 'identified', 'implements', 'improved', 'including', 'infections', 'inhaling', 'introduced', 'invented', 'inventing', 'inventions', 'invited', 'items', 'joined', 'kidding', 'kuemper', 'legislators', 'lighted', 'lives', 'll', 'looks', 'loved', 'lungs', 'makes', 'markets', 'medicated', 'medications', 'members', 'menthols', 'mentioned', 'michael', 'months', 'moves', 'mr', 'needed', 'nestled', 'notes', 'offered', 'offers', 'oozing', 'organisations', 'others', 'packaging', 'packing', 'packs', 'paid', 'parents', 'passed', 'paul', 'photos', 'pictures', 'pillows', 'pioneered', 'placed', 'places', 'pm', 'posts', 'powers', 'predicted', 'proceeded', 'produces', 'producing', 'products', 'promoting', 'protected', 'prototypes', 'proud', 'provides', 'published', 'purchasing', 'pursuing', 'quacks', 'reached', 'reared', 'recommendations', 'recruited', 'referring', 'regards', 'relatives', 'reported', 'revamped', 'rewards', 'rights', 'ripped', 'roadmap', 'roamed', 'saviour', 'says', 'screamed', 'searched', 'shakedly', 'shared', 'shouted', 'showed', 'smokers', 'solved', 'staples', 'started', 'starts', 'stored', 'styrofoam', 'supplies', 'swims', 'taxes', 'teenager', 'tells', 'texting', 'things', 'thoughts', 'tis', 'tutorials', 'types', 'uk', 'unforgivig', 'updated', 'updating', 'using', 'vape', 'vaper', 'vapers', 'vaping', 'vapour', 've', 'ventured', 'versions', 'viewing', 'views', 'voices', 'walks', 'wanted', 'wants', 'warnings', 'wasn', 'words', 'www', 'years', 'yielded']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution uses the text_from_web function from exercise 8.\n",
    "\n",
    "def unknown(url):\n",
    "    raw = text_from_web(url)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    words = sorted(set(words))\n",
    "    wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "    unknown_words = [word for word in words if word not in wordlist]\n",
    "    return unknown_words\n",
    "\n",
    "unknown('https://www.ecigarettedirect.co.uk/ashtray-blog/2013/10/interview-inventor-e-cigarette-herbert-a-gilbert.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista de palabras proporcionada por `nltk.corpus.words` siendo bastante incompleta, la mayoría de las palabras \"desconocidas\" son en realidad palabras totalmente válidas y comunes ('credits', 'box', 'looks', etc). Solo en algunos casos se trata de palabras mal escritas ('cigerette'), de trozos de html que BeautifulSoup no ha logrado eliminar ('endif'), de nombres propios escritos en minúsculas ('uk') o de partes de contracciones ('didn')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'smoke', '.', '¿', '¡', 'Do', \"n't\", 'you', 'know', '?', '!']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I don't smoke.     ¿¡Don't you know?!\"\n",
    "matches = re.findall(r\"([Dd]o)(n't)|(\\w+)|(\\S)\", sent)\n",
    "\n",
    "matches = [token for match in matches for token in match if token]\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión `«n't|\\w+»` no funciona. Si la aplicamos a \"don't\", tanto \"n't\" como \"don\" son resultados válidos. Sin embargo, los elementos que devuelve `findall` nunca se solapan, entonces solo se devolverá uno de ello. Después de haberlo probado con más cadenas, he llegado a la conclusión de que se prioriza el elemento más cercano al principio de la cadena. En este caso, es \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pzzth0n 1s an 1nt3rpr3t3stu,  7 0bj3ct-0r13nt3stu, h1gh-|3v3| pr0gramm1ng |anguag3 w1th stuzznam1c $3mant1cs5w33t! \\n1ts h1gh-|3v3| bu1|t 1n stuata $tructur3s, c0mb1n3stu w1th stuzznam1c tzzp1ng anstu stuzznam1c b1nstu1ng, \\nmak3 1t v3rzz attract1v3 f0r rap1stu app|1cat10n stu3v3|0pm3nt, as w3|| as f0r u53 as a $cr1pt1ng \\n0r g|u3 |anguag3 t0 c0nn3ct 3x15t1ng c0mp0n3nts t0g3th3r5w33t! 1 at3 a $k1nnzz pzzth0n5w33t!'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "    \n",
    "text = \"\"\"Python is an interpreted,  7 object-oriented, high-level programming language with dynamic semantics. \n",
    "Its high-level built in data structures, combined with dynamic typing and dynamic binding, \n",
    "make it very attractive for Rapid Application Development, as well as for use as a scripting \n",
    "or glue language to connect existing components together. I ate a skinny python.\"\"\"\n",
    "\n",
    "dic = {\n",
    "    r'e': r'3',\n",
    "    r'i': r'1',\n",
    "    r'o': r'0',\n",
    "    r'l': r'|',\n",
    "    r'\\bs': r'$',\n",
    "    r'(?P<start>\\w+)s(?P<end>\\w+)': r'\\g<start>5\\g<end>', #word-end s is not considered\n",
    "    r'\\.': r'5w33t!',\n",
    "    r'ate': r'8',\n",
    "    r'd': r'stu',\n",
    "    r'y': r'zz'\n",
    "}\n",
    "\n",
    "multiple_replace(text.lower(), dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "    Write a function to convert a word to Pig Latin.\n",
    "    Write code that converts text, instead of individual words.\n",
    "    Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting words to Pig Latin:\n",
      "look --> ooklay\n",
      "stamp --> ampstay\n",
      "GRound --> oundGRay\n",
      "Cherry --> Errychay\n",
      "idle --> idleay\n",
      "Quick --> Ickquay\n",
      "qgo --> oqgay\n",
      "squeeze --> eezesquay\n",
      "yellow --> ellowyay\n",
      "style --> ylestay\n",
      "You --> Ouyay\n",
      "SYD --> YDSay\n",
      "yll --> yllay\n",
      "oe --> oeay\n",
      "\n",
      "\n",
      "Converting a sentence into Pig Latin:\n",
      "This is quite a long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\n",
      "------->\n",
      "Isthay isay itequay aay onglay \"entencesay\" 5+2... Esyay, itay isay!!! ¿Arenay'tay erethay wotay entencessay erehay?\n"
     ]
    }
   ],
   "source": [
    "def word_to_pig(word):\n",
    "    if not word.isalpha():\n",
    "        return word\n",
    "    pattern = re.compile(r'^((([b-df-hj-np-t-zB-DF-HJ-NP-T-Z])*[qQ][uU])|[yY]?[b-df-hj-np-t-zB-DF-HJ-NP-T-Z]*)?(\\w*)')\n",
    "    # TODO: referring to ([b-df-hj-np-t-zB-DF-HJ-NP-T-Z]) as \\3 isn't working. Why?\n",
    "    pig_word = re.sub(pattern, \n",
    "                      r'\\4\\1ay', \n",
    "                      word)\n",
    "    if word.istitle() and len(pig_word) > 1:\n",
    "        pig_word = pig_word[0].title() + pig_word[1:].lower()\n",
    "    return pig_word\n",
    "\n",
    "def tokenize_text(raw):\n",
    "    tokens= re.findall(r\"[a-zA-Z]+|\\S|\\s\", raw)\n",
    "    return tokens\n",
    "\n",
    "def sent_to_pig(sent):\n",
    "    tokens = tokenize_text(sent)\n",
    "    pig_tokens = [word_to_pig(token)  \n",
    "                  for token in tokens]\n",
    "    pig_sent = ''.join(pig_tokens)\n",
    "    return pig_sent\n",
    "\n",
    "\n",
    "words = ['look', 'stamp', 'GRound', 'Cherry', 'idle', 'Quick', 'qgo',\n",
    "         'squeeze', 'yellow', 'style', 'You', 'SYD', 'yll', 'oe']\n",
    "sent = \"\"\"This is quite a long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\"\"\"\n",
    "\n",
    "print('Converting words to Pig Latin:')\n",
    "for word in words:\n",
    "    print('{} --> {}'.format(word, word_to_pig(word)))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Converting a sentence into Pig Latin:\")\n",
    "print(sent)\n",
    "print('------->')\n",
    "print(sent_to_pig(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a e i u á é \n",
      "a 0 0 1 1 0 0 \n",
      "e 1 1 1 0 0 1 \n",
      "i 1 1 1 0 1 0 \n",
      "á 0 0 0 0 0 1 \n",
      "ó 1 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "raw = ' '.join(nltk.corpus.udhr.words('Hungarian_Magyar-Latin1'))\n",
    "vowel_seqs = set(re.findall(r'[aeiouAEIOUáéíóöőúüűÁÉÍÓÖŐÚÜŰ]{2,}', raw))\n",
    "vowel_seqs = [tuple(seq) for seq in vowel_seqs]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(vowel_seqs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hea heaaheaa ahahaa ahahhh hahahh ehe ahehhah hhhha a a a hh heeahaehhh hehe haheh hhahea hah hh a hae hhaee ahhh ea h e hh hh hhhahahaahhahahheeeheahehh a eaah hah e heahhhehh ha h e hahaha he eahahhhhaa hhhhhaa aahh a hhhe hhaah ee aaa hhe ehh e eehehheeeeahheeea hehheeehhha eh aeaeahehhh hehhhh hhae hhaahhhheahhhheehahahhaaeaeeaea a ahaheehaheaehaehea hhehh a haee aahhh eheh eha h hehheeehhhhhhahhehe ehh hae h ahhhhh hh heaahahhhh h hhhha h eaahhaaeaah aeeae hhah haheheeh ahhe\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "chars = 'aehh '\n",
    "text = ''.join([choice(chars) for i in range(0,500)])\n",
    "text = ' '.join(text.split())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8295405382019556 14.95406512831572\n",
      "For the adventure section of the Brown corpus, the ARI index equals to 4.08.\n",
      "4.333780098904654 24.01109723956166\n",
      "For the belles_lettres section of the Brown corpus, the ARI index equals to 10.99.\n",
      "4.3786442438802675 20.555221888555224\n",
      "For the editorial section of the Brown corpus, the ARI index equals to 9.47.\n",
      "3.8813514776311178 16.118616144975288\n",
      "For the fiction section of the Brown corpus, the ARI index equals to 4.91.\n",
      "4.660610123079995 23.12565963060686\n",
      "For the government section of the Brown corpus, the ARI index equals to 12.08.\n",
      "4.359451089926528 19.638683520152636\n",
      "For the hobbies section of the Brown corpus, the ARI index equals to 8.92.\n",
      "4.037427978796958 20.60303893637227\n",
      "For the humor section of the Brown corpus, the ARI index equals to 7.89.\n",
      "4.585354723786066 23.51797258856995\n",
      "For the learned section of the Brown corpus, the ARI index equals to 11.93.\n",
      "4.328226003862229 22.59762343782012\n",
      "For the lore section of the Brown corpus, the ARI index equals to 10.25.\n",
      "3.8020780492924486 14.71152856407617\n",
      "For the mystery section of the Brown corpus, the ARI index equals to 3.83.\n",
      "4.401545438271973 21.75081116158339\n",
      "For the news section of the Brown corpus, the ARI index equals to 10.18.\n",
      "4.278814183101094 22.95979020979021\n",
      "For the religion section of the Brown corpus, the ARI index equals to 10.2.\n",
      "4.3687106918239 23.246145059965734\n",
      "For the reviews section of the Brown corpus, the ARI index equals to 10.77.\n",
      "3.795721344720231 15.80275332881968\n",
      "For the romance section of the Brown corpus, the ARI index equals to 4.35.\n",
      "3.986454733932274 15.263713080168776\n",
      "For the science_fiction section of the Brown corpus, the ARI index equals to 4.98.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "def compute_ari(category):\n",
    "    \"\"\"\n",
    "    Computes the average readability of a text based on\n",
    "    average word length and average sentences length.\n",
    "    \"\"\"    \n",
    "    muw, mus = get_muw_mus(category)\n",
    "    ari = 4.71 * muw + 0.5 * mus - 21.43\n",
    "    return ari\n",
    "\n",
    "def compute_average_len(elements):\n",
    "    \"\"\"\n",
    "    Compute the average length of the elements of a list.\n",
    "    Can be used both for a list of lists and for a list of strings.\n",
    "    \"\"\"\n",
    "    sum_lens = sum([len(elm) for elm in elements])\n",
    "    return sum_lens / len(elements)\n",
    "\n",
    "def get_muw_mus(category):\n",
    "    \"\"\"\n",
    "    Returns the average lengths of the words (muw) \n",
    "    and the sentences (mus) for a specific category of the Brown corpus.\n",
    "    \"\"\"\n",
    "    muw = compute_average_len(brown.words(categories=category))\n",
    "    mus = compute_average_len(brown.sents(categories=category))\n",
    "    return (muw, mus)\n",
    "\n",
    "for category in brown.categories():\n",
    "    ari = compute_ari(category)\n",
    "    print('For the {} section of the Brown corpus, the ARI index equals to {}.'.format(category, round(ari, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "['the', 'click', 'OF', 'cuthbert', '***', 'produc', 'by', 'suzann', 'L.', 'shell', ',', 'charl', 'frank', 'and', 'the', 'onlin', 'distribut', 'proofread', 'team', 'the', 'click', 'OF', 'cuthbert', 'by', 'P.', 'G.', 'wodehous', '1922', 'dedic', 'TO', 'the', 'immort', 'memori', 'OF', 'john', 'henri', 'and', 'pat', 'rogi', 'who', 'AT', 'edinburgh', 'IN', 'the', 'year', '1593', 'a.d', '.', 'were', 'imprison']\n",
      "Lancaster Stemmer:\n",
      "['the', 'click', 'of', 'cuthbert', '***', 'produc', 'by', 'suzan', 'l.', 'shel', ',', 'charl', 'frank', 'and', 'the', 'onlin', 'distribut', 'proofread', 'team', 'the', 'click', 'of', 'cuthbert', 'by', 'p.', 'g.', 'wodeh', '1922', 'ded', 'to', 'the', 'immort', 'mem', 'of', 'john', 'henry', 'and', 'pat', 'rog', 'who', 'at', 'edinburgh', 'in', 'the', 'year', '1593', 'a.d', '.', 'wer', 'imprison']\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "stemmed_porter = [porter.stem(token) for token in tokens]\n",
    "stemmed_lancaster = [lancaster.stem(token) for token in tokens]\n",
    "\n",
    "print('Porter Stemmer:')\n",
    "print(stemmed_porter[:50])\n",
    "print('Lancaster Stemmer:')\n",
    "print(stemmed_lancaster[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El stemmer de Lancaster lo pasa todo a minúsculas, mientras que el Porter no lo hace. También el Lancaster considera como terminación todo lo que en teoría podría serlo. Por ejemplo, \"Wodehouse\" -> \"Wodeh\", \"memory\" -> \"mem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "\n",
    "lengths = []\n",
    "for word in saying:\n",
    "    lengths.append(len(word))\n",
    "    \n",
    "lengths2 = [len(word) for word in saying]\n",
    "\n",
    "print(lengths2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "    Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.\n",
    "    Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "    Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "    Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second characters: eoldrnnnna\n",
      "Sorted words:\n",
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = silly.split()\n",
    "second_chars = ''.join([word[1] for word in bland if len(word) >= 2])\n",
    "print('Second characters: {}'.format(second_chars))\n",
    "new_silly = ' '.join(bland)\n",
    "\n",
    "print('Sorted words:')\n",
    "for word in sorted(bland):\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "    What happens when you look up a substring, e.g. 'inexpressible'.index('re')?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando buscamos una subcadena con `index`, se devuelve la posición del primer caracter de la subcadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "    Define a variable silly as in the exercise above. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newly formed bland ideas are inexpressible\n"
     ]
    }
   ],
   "source": [
    "words = ['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'said', 'than', 'done']\n",
    "said_index = words.index('said')\n",
    "\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "silly_split = silly.split()\n",
    "i = silly_split.index('in')\n",
    "silly_sliced = ' '.join(silly_split[:i])\n",
    "\n",
    "print(silly_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(n) Mexican person usually lives in Mexico.\n",
      "A(n) Russian person usually lives in Russian Federation.\n",
      "A(n) Portuguese person usually lives in Portugal.\n",
      "A(n) Swedish person usually lives in Sweden.\n",
      "A(n) Irish person usually lives in Iran, Islamic Republic of.\n",
      "A(n) Kazakh person usually lives in Kazakhstan.\n",
      "Pythonese is not a nationality that has a state.\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "def nation_to_state(nation):\n",
    "    \"\"\"\n",
    "    Strips out the endings of the adjectives, matches the remaining stem\n",
    "    against a list of states and return the first state that starts with the stem.\n",
    "    If no match is found, None is returned.\n",
    "    \"\"\"\n",
    "    countries = [country.name for country in pycountry.countries]\n",
    "    pattern = re.compile(r'(^\\w+?)(?:ian|an|uese|se|ish|kh)')\n",
    "    nation_stem = re.findall(pattern, nation)[0]\n",
    "\n",
    "    if not nation_stem:\n",
    "        state = None\n",
    "    else:\n",
    "        state = None\n",
    "        for country in countries:\n",
    "            if country.startswith(nation_stem):\n",
    "                state = country\n",
    "                break\n",
    "    return state\n",
    "\n",
    "nations = ['Mexican', 'Russian', 'Portuguese', 'Swedish', 'Irish', 'Kazakh', 'Pythonese']\n",
    "\n",
    "for nation in nations:\n",
    "    state = nation_to_state(nation)\n",
    "    if not state:\n",
    "        print ('{} is not a nationality that has a state.'.format(nation))\n",
    "    else:\n",
    "        print('A(n) {} person usually lives in {}.'. format(nation, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existe un único patrón para la formación de nombres nacionalidades a partir de nombres de estados, por eso el modelo que propongo es imperfecto y cualquier otro que propongamos también lo será. Por ejemplo, para \"Irish\" la función devolverá \"Iran\", ya que en la lista de estados \"Iran\" este país viene antes de \"Ireland\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in 3.5. http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best they can\n",
      "the best he could\n",
      "\n",
      "as best he could; as best I could; as best I could\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "tokens = nltk.Text(brown.words())\n",
    "\n",
    "starts = ['<the> <best>', '<as> <best>', '<as> <best> <as>']\n",
    "ends = ['<can>', '<could>']\n",
    "\n",
    "for start in starts:\n",
    "    for end in ends:\n",
    "        search_string = r'' + start + '<I|you|he|she|it|we|they>' + end\n",
    "        tokens.findall(search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todo el corpus de Brown, solo encontramos un caso de tipo \"the best p can\", uno de \"the best p could\" y 3 casos de \"as best p could\", pero ninguno de \"as best p can\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 36."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hai, ai liek your plafe. Coem wif me toniet, fe niet is ovah.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"\"\"Hi, I like your plathe. Come with me tonight, the night is over.\"\"\"\n",
    "\n",
    "dic = {\n",
    "    r'dude': r'dood',\n",
    "    r'(?P<start>\\b[a-zA-Z]+)(?P<cons>[^aeiouh\\d\\s])e\\b': r'\\g<start>e\\g<cons>',\n",
    "    # can't find a way to exclude the 'th' sequence from the cons group.\n",
    "    r'(?P<start>\\b[a-zA-Z]*)[Ii]\\b': r'\\g<start>ai',\n",
    "    r'ight': r'iet',\n",
    "    r'(?P<start>\\b[a-zA-Z]+)er\\b': r'\\g<start>ah',\n",
    "    r'[T][hH]': r'F',\n",
    "    r'[t][hH]': r'f'\n",
    "}\n",
    "\n",
    "multiple_replace(text, dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He implementado esta parte de las reglas de lolcat:\n",
    "\n",
    "In lolcat speak, a lot of words are misspelled. Think baby talk, but with a cat flair. This is probably one of the hardest parts to get down correctly. Below are some common misspelling patterns.\n",
    "\n",
    "\"eye\" sounds are almost always rendered into ai, unless it just doesn't fit into the word. Most words that end in the \"eye\" sound can benefit from this set up. However, it appears that \"ight\" configurations don't benefit from using ai. Instead, change \"ight\" to \"iet\".\n",
    "\n",
    "    Hi -> hai\n",
    "    I -> ai \n",
    "\n",
    "Words that end in a silent \"e\" that have a consonant before often exchange the two last letters. However, it works better for some words than for others, so some discretion is advisable.\n",
    "\n",
    "    kite -> kiet\n",
    "    like -> liek\n",
    "    come -> coem\n",
    "    came -> caem\n",
    "    bake -> baek\n",
    "    plate -> plaet \n",
    "\n",
    "A good exception to this rule is \"dude\" which is rendered into \"dood\" or \"d00d\". \n",
    "\n",
    "Another note of interest are words that end in \"er\". Either you'll end up dropping the \"e\", replacing it with \"u\", or changing the ending to \"ah\". Both ways are acceptable.\n",
    "\n",
    "    over -> ovah, ovur, ovr\n",
    "    hover -> hovah, hovr\n",
    "    peeker -> peekah, peekr \n",
    "\"Th\" sounds are usually replaced with \"f\", but can occasionally be replaced with \"tt\" or \"dd\". Rarely will the \"th\" be kept.\n",
    "\n",
    "    Nothing -> Nofin, nuttin, etc\n",
    "    Three -> tree, free, fwee, twee\n",
    "    With -> wif     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample \"Hello, World\" Application Sample \"Hello, World\" Application This is the home page for the HelloWorld Web application. To prove that they work, you can execute either of the following links: To a JSP page. To a servlet.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/3_37.html') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "\n",
    "patterns = {\n",
    "            re.compile(r'<.+?>'): '',\n",
    "            re.compile(r'\\s{2,}') : ' '\n",
    "}\n",
    "\n",
    "for pattern, new in patterns.items():\n",
    "    html = pattern.sub(new, html)\n",
    "\n",
    "raw = html.strip()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting challenge for tokenization is words that have been split across a line-break. E.g. if long-term is split, then we have the string long-\\nterm.\n",
    "\n",
    "    Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "    Use re.sub() to remove the \\n character from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world long-term commitment a rock-hard cake my friend from pla-net earth Jean-Paul is a priest, he's got an amazing encyclo-pedia.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hello world long-\n",
    "term commitment a rock-\n",
    "hard cake my friend from pla-\n",
    "net earth Jean-\n",
    "Paul is a priest, he's got an amazing encyclo-\n",
    "pedia.\"\"\"\n",
    "\n",
    "pattern = re.compile(r'\\b((?P<start>[a-zA-Z]+)-\\n(?P<end>[a-zA-Z]+))\\b')\n",
    "new_text = pattern.sub(r'\\g<start>-\\g<end>', text)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How might you identify words that should not remain hyphenated once the newline is removed, e.g. 'encyclo-\\npedia'?x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world long-term commitment a rock-hard cake my friend from planet earth Jean-Paul is a priest, he's got an amazing encyclopedia.\n"
     ]
    }
   ],
   "source": [
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "\n",
    "def check_hyphen(match):\n",
    "    if (match.group(2) + match.group(3)) in wordlist:\n",
    "        return '{}{}'.format(match.group(2), match.group(3))\n",
    "    return '{}-{}'.format(match.group(2), match.group(3))\n",
    "\n",
    "smart_text = re.sub(pattern, check_hyphen, text)\n",
    "print(smart_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 39."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
    "\n",
    "    Retain the first letter of the name and drop all other occurrences of a, e, i, o, u, y, h, w.\n",
    "    Replace consonants with digits as follows (after the first letter):\n",
    "        b, f, p, v → 1\n",
    "        c, g, j, k, q, s, x, z → 2\n",
    "        d, t → 3\n",
    "        l → 4\n",
    "        m, n → 5\n",
    "        r → 6\n",
    "    If two or more letters with the same number are adjacent in the original name (before step 1), only retain the first letter; also two letters with the same number separated by 'h' or 'w' are coded as a single number, whereas such letters separated by a vowel are coded twice. This rule also applies to the first letter.\n",
    "    If you have too few letters in your word that you can't assign three numbers, append with zeros until there are three numbers. If you have more than 3 letters, just retain the first 3 numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashcroft: A261\n",
      "Ashcraft: A261\n",
      "Robert: R163\n",
      "Rupert: R163\n",
      "Rubin: R150\n",
      "Tymczak: T522\n",
      "Ocgojab: O221\n",
      "Pfister: P236\n",
      "Honeyman: H555\n",
      "Walterson: W436\n",
      "Amina: A550\n",
      "Tatiana: T350\n",
      "Twtiana: T500\n",
      "Krrristoffferson: K623\n"
     ]
    }
   ],
   "source": [
    "names = ['Ashcroft', 'Ashcraft', 'Robert', 'Rupert', 'Rubin', 'Tymczak', 'Ocgojab', \n",
    "         'Pfister', 'Honeyman','Walterson', 'Amina', 'Tatiana', 'Twtiana', 'Krrristoffferson']\n",
    "\n",
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "dic = {\n",
    "    r'[bfpvBFPV]': r'1',\n",
    "    r'[cgjkqsxzCGJKQSXZ]': r'2',\n",
    "    r'[dtDT]': r'3',\n",
    "    r'[lL]': r'4',\n",
    "    r'[mnMN]': r'5',\n",
    "    r'[rR]': r'6'\n",
    "}\n",
    "\n",
    "def to_soundex(name):\n",
    "    first_letter = name[0]\n",
    "    # substitute consonants with numbers\n",
    "    name = multiple_replace(name, dic)\n",
    "    # for two or more same numbers in a row or separated by 'h' or 'w', only keep the first number.\n",
    "    name = re.sub(r'(([1-6])[hw]?\\2+)+', r'\\2', name)\n",
    "    # strip out first letter and all non-digits\n",
    "    name = re.sub(r'[^1-6]', '', name[1:])\n",
    "    # retain first 3 digits, if < 3 append zeros\n",
    "    name = name[:3].ljust(3, '0')\n",
    "    name = first_letter + name\n",
    "    return name\n",
    "\n",
    "for name in names:\n",
    "    soundex = to_soundex(name)\n",
    "    print('{}: {}'. format(name, soundex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the rural genre of the ABC corpus, the ARI index equals to 12.62.\n",
      "For the science genre of the ABC corpus, the ARI index equals to 12.77.\n"
     ]
    }
   ],
   "source": [
    "def compute_ari(genre):\n",
    "    \"\"\"\n",
    "    Computes the average readability of a text based on\n",
    "    average word length and average sentences length.\n",
    "    \"\"\"    \n",
    "    muw, mus = get_muw_mus(genre)\n",
    "    ari = 4.71 * muw + 0.5 * mus - 21.43\n",
    "    return ari\n",
    "\n",
    "def compute_average_len(elements):\n",
    "    \"\"\"\n",
    "    Compute the average length of the elements of a list.\n",
    "    Can be used both for a list of lists and for a list of strings.\n",
    "    \"\"\"\n",
    "    sum_lens = sum([len(elm) for elm in elements])\n",
    "    return sum_lens / len(elements)\n",
    "\n",
    "def get_muw_mus(genre):\n",
    "    \"\"\"\n",
    "    Returns the average lengths of the words (muw) \n",
    "    and the sentences (mus) for a specific genre of the ABC corpus.\n",
    "    \"\"\"\n",
    "    sents = tokenize_sents(genre)\n",
    "    words = [word for sent in sents for word in sent]\n",
    "    muw = compute_average_len(words)\n",
    "    mus = compute_average_len(sents)\n",
    "    return (muw, mus)\n",
    "\n",
    "def tokenize_sents(genre):\n",
    "    raw = nltk.corpus.abc.raw(genre + '.txt')\n",
    "    sents = [nltk.word_tokenize(sent) \n",
    "            for sent in nltk.sent_tokenize(raw)]\n",
    "    return sents\n",
    "    \n",
    "for genre in ['rural', 'science']:\n",
    "    ari = compute_ari(genre)\n",
    "    print('For the {} genre of the ABC corpus, the ARI index equals to {}.'.format(genre, round(ari, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "\n",
    "# Original loop:\n",
    "#     >>> vsequences = set()\n",
    "#     >>> for word in words:\n",
    "#     ...     vowels = []\n",
    "#     ...     for char in word:\n",
    "#     ...         if char in 'aeiou':\n",
    "#     ...             vowels.append(char)\n",
    "#     ...     vsequences.add(''.join(vowels))\n",
    "#     >>> sorted(vsequences)\n",
    "#     ['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "\n",
    "# List comprehension:\n",
    "vsequences = [re.sub(r'[b-df-hj-np-t-z]', r'', word) for word in words]\n",
    "sorted(vsequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ourt at Camelot . I must speak with your lord and master . SOLDIER # 1 : What ?  \n",
      "e live there . ARTHUR : Then who is your lord ? WOMAN : We don ' t have a        \n",
      "        lord ? WOMAN : We don ' t have a lord . ARTHUR : What ? DENNIS : I told  \n",
      "rom the bosom of the water signifying by Divine Providence that I , Arthur , was \n",
      "ory is mine ! [ kneeling ] We thank Thee Lord , that in Thy mer -- BLACK KNIGHT :\n",
      "     ARTHUR : I ' m averting my eyes , O Lord . GOD : Well , don ' t .           \n",
      "     . Now knock it off ! ARTHUR : Yes , Lord . GOD : Right ! Arthur , King of   \n",
      "hese dark times . ARTHUR : Good idea , O Lord ! GOD : ' Course it ' s a          \n",
      "CELOT : A blessing ! A blessing from the Lord ! GALAHAD : God be praised SCENE 8 \n",
      "he hand grenade up on high , saying ,' O Lord , bless this thy hand grenade that \n",
      "  to tiny bits , in thy mercy .' And the Lord did grin , and the people did feast\n",
      "bit , Brother . SECOND BROTHER : And the Lord spake , saying , ' First shalt thou\n",
      "   quest is at an end ! God be praised ! Almighty God , we thank Thee that Thou h\n",
      "tom biters . ARTHUR : In the name of the Lord , we demand entrance to this sacred\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index_stem = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "        self._index_synset = nltk.Index((self._synset(word), i)\n",
    "                                 for (i, word) in enumerate(text)\n",
    "                                 if self._synset(word))\n",
    "\n",
    "    def concordance(self, key, indexer, width):\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in indexer[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "            \n",
    "    def concordance_stem(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        return self.concordance(key, self._index_stem, width)\n",
    "    \n",
    "    def concordance_synset(self, word, width=40):\n",
    "        key = self._synset(word)\n",
    "        return self.concordance(key, self._index_synset, width)        \n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "    def _synset(self, word):\n",
    "        try:\n",
    "            a = wn.synsets(word)[0].offset\n",
    "            return a\n",
    "        except IndexError:\n",
    "            None\n",
    "\n",
    "    \n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance_synset('creator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si buscamos concordancias de 'creator' con el índice de synsets, se nos devuelven concordancias de las palabras \"lord\", \"Divine\" y \"Almighty God\", porque éstan, al igual que \"creator\" forman parte del synset 'godhead.n.01'.\n",
    "Una cosa que no entiendo es por qué se nos aconseja usar el offset del synset en vez del propio synset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 43."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French is the winner with a coefficient of 0.9551841975557196.\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "def get_bigram_fd(words):\n",
    "    \"\"\"\n",
    "    From a wordlist returns a relative frequency distribution\n",
    "    of letter pairs.\n",
    "    \"\"\"\n",
    "    letter_bigrams = [''.join(double) \n",
    "                     for word in words \n",
    "                     for double in zip(word.lower(), word.lower()[1:])\n",
    "                     if double[0].isalpha() and double[1].isalpha()]\n",
    "    fd = nltk.FreqDist(letter_bigrams)\n",
    "    rel_fd = get_rel_fd(copy(fd))\n",
    "    return rel_fd\n",
    "\n",
    "def get_rel_fd(fd, i=10000):\n",
    "    \"\"\"\n",
    "    In a frequency distribution, replace absolute frequencies\n",
    "    by relative frequencies per every i items.\n",
    "    \"\"\"\n",
    "    length = fd.N()\n",
    "    for key, value in fd.items():\n",
    "        fd[key] = round(value / length * i, 8)\n",
    "    return fd\n",
    "\n",
    "# get a text in a \"mystery\" language\"\n",
    "url = \"http://www.gutenberg.org/cache/epub/18812/pg18812.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "first = raw.find(\"Il semble qu'en un temps comme le nôtre,\")\n",
    "last = raw.rfind(\"NOTE DU TRANSCRIPTEUR\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "# tokenize the \"mystery\" text\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "# get a freqdist for the \"mystery text\"    \n",
    "mystery_fd = get_bigram_fd(tokens)\n",
    "\n",
    "languages = ['French_Francais-Latin1', 'Icelandic_Yslenska-Latin1', 'Basque_Euskara-Latin1',\n",
    "            'English-Latin1', 'Italian-Latin1']\n",
    "\n",
    "# measure spearman correlation between the \"mystery\" fd and each language fd\n",
    "ranks = {}\n",
    "for language in languages:\n",
    "    language_fd = get_bigram_fd(nltk.corpus.udhr.words(language))\n",
    "    rank = nltk.spearman_correlation(mystery_fd, language_fd)\n",
    "    ranks[language] = rank\n",
    "# get the language with the highest correlation coefficient\n",
    "mystery_language = max(ranks, key=ranks.get)\n",
    "top_coefficient = ranks[mystery_language]\n",
    "pos = mystery_language.find('_')\n",
    "mystery_language = mystery_language[:pos]\n",
    "\n",
    "\n",
    "print('{} is the winner with a coefficient of {}.'.format(mystery_language, top_coefficient))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
