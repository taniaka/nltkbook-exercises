{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK BOOK. Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from random import choice\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dish\n",
      "run\n",
      "nation\n",
      "do\n",
      "heat\n",
      "do\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "def remove_affix(word, suffix=True, prefix=False):\n",
    "    if suffix:\n",
    "        hyphen = word.rfind('-')\n",
    "        word = word[:hyphen]\n",
    "    if prefix:\n",
    "        hyphen = word.find('-')\n",
    "        word = word[hyphen+1:]\n",
    "    return word\n",
    "\n",
    "print(remove_affix('dish-es'))\n",
    "print(remove_affix('run-ning'))\n",
    "print(remove_affix('nation-ality'))\n",
    "print(remove_affix('un-do', False, True))\n",
    "print(remove_affix('pre-heat', False, True))\n",
    "print(remove_affix('un-do-ing', prefix=True))\n",
    "print(remove_affix('cat', suffix=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like there's an Index error here.\n"
     ]
    }
   ],
   "source": [
    "word = \"NLTK\"\n",
    "\n",
    "try:\n",
    "    i = word[-6] # 6th character from the right doesn't exist, so an IndexError is returned.\n",
    "except IndexError:\n",
    "    print(\"Looks like there's an Index error here.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = 'Monty Python'\n",
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los dos primeros parámetros no están indicados, se reproduce la cadena entera. El tercer parámetro indica cada cuántos caracteres queremos reproducir. Con `1` indicamos que nos interesan todos los caracteres, y como el valor es negativo, la cadena se va a leer desde el final.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "    [a-zA-Z]+                      uno o más caracteres alpha\n",
    "    [A-Z][a-z]*                    una mayúscula seguida de zero, una o más minúsculas\n",
    "    p[aeiou]{,2}t                  una 'p' seguida de zero, una o dos vocales seguida(s) de una 't'\n",
    "    \\d+(\\.\\d+)?                    uno o más dígitos seguidos o no de (un punto seguido de uno o más dígitos) \n",
    "    ([^aeiou][aeiou][^aeiou])*     (nada) o una o más secuencias de (un caracter que no sea una vocal seguido de una    \n",
    "                                   vocal seguida de uncaracter que no sea una vocal)\n",
    "    \\w+|[^\\w\\s]+                   (uno o más caracteres alfanuméricos) o (uno o más caracteres que no sean  ni\n",
    "                                   alfanuméricos ni de tipo whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "\n",
    "        1. A single determiner (assume that a, an, and the are the only determiners)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'an', 'a', 'The']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = r'\\b([aA][n]?|[tT]he)\\b'\n",
    "re.findall(regexp, 'the tank Anna scandal then an a April The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       2. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9+23*45', '10*23+45', '666*2+14', '600*93+5', '700+7*2']\n"
     ]
    }
   ],
   "source": [
    "# re doesn't support overlapping, but regex does\n",
    "\n",
    "import regex\n",
    "\n",
    "pattern = regex.compile(r'(?:^|\\s)(\\d+(?:\\*\\d+\\+|\\+\\d+\\*)\\d+)(?:\\s|[\\.,!\\?]+|$)')\n",
    "#  matches expressions that\n",
    "    # contain one '+' and one '*', in any order\n",
    "    # open the string or are preceded by a whitespace\n",
    "    # close the string or are followed by a whitespace or punctuation mark.\n",
    "    \n",
    "seq = '9+23*45 10*23+45?! 666*2+14,p 9*237+11+*3+2 1+2+4  600*93+5 700+7*2.'\n",
    "\n",
    "matches = regex.findall(pattern, seq, overlapped=True)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLTK 3.2.5 documentation next | modules | index Natural Language Toolkit ¶ NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum . Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.” Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The book is being updated for Python 3 and NLTK 3. (The original Python 2 version is still available at http://nltk.org/book_1ed .) Some simple things you can do with NLTK ¶ Tokenize and tag some text: >>> import  nltk >>> sentence  =  \"\"\"At eight o\\'clock on Thursday morning ... Arthur didn\\'t feel very good.\"\"\" >>> tokens  =  nltk . word_tokenize ( sentence ) >>> tokens [\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\', \\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\'] >>> tagged  =  nltk . pos_tag ( tokens ) >>> tagged [ 0 : 6 ] [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')] Identify named entities: >>> entities  =  nltk . chunk . ne_chunk ( tagged ) >>> entities Tree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'), Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]), (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'), (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')]) Display a parse tree: >>> from  nltk.corpus  import  treebank >>> t  =  treebank . parsed_sents ( \\'wsj_0001.mrg\\' )[ 0 ] >>> t . draw () NB. If you publish work that uses NLTK, please cite the NLTK book as follows: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python .  O’Reilly Media Inc. Next Steps ¶ sign up for release announcements join in the discussion Contents ¶ NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Index Module Index Search Page Table Of Contents NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Search next | modules | index Show Source © Copyright 2017, NLTK Project.       Last updated on Sep 24, 2017.       Created using Sphinx 1.5.5.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution is mostly copied from Ranveer Aggarwal's answer to https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_web(url, decoder='utf8'):\n",
    "    html = request.urlopen(url).read().decode(decoder)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    data = soup.findAll(text=True)\n",
    "    lines = filter(visible, data)\n",
    "    lines = [line.strip().replace('\\n', ' ') for line in lines if line != '\\n']\n",
    "    raw = ' '.join(list(lines))\n",
    "    return raw\n",
    "\n",
    "text_from_web('http://nltk.org/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', ']', ',', ',', '.', '—', ',', '.', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.', '.', ',', ',', '.', ',', '(', '.', ')', '—', '—', ',', ',', ',', ',', ',', '.', ',', '.', ',', '.', '.', ',']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(filepath):\n",
    "    with open(filepath) as f:\n",
    "        raw = ' '.join([line.strip() for line in f.readlines()])\n",
    "    return raw\n",
    "\n",
    "pattern = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "          [^\\s\\w]   # get punctuation         \n",
    "\"\"\"\n",
    "\n",
    "raw = load('../data/corpus.txt')\n",
    "tokenized = nltk.regexp_tokenize(raw, pattern)\n",
    "tokenized[:50] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John-Michael Doe', 'Chicago', 'U.S.A.', '$ 2500.8', '€2012,6', 'Spain', '6/7/2018']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2 = \"\"\"My friend John-Michael Doe from Chicago in the U.S.A. bought a house for $ 2500.8 (i.e. €2012,6)\n",
    "       in Spain on 6/7/2018.\"\"\"\n",
    "\n",
    "pattern2 = r\"\"\"(?x)  # set flag to allow verbose regexps\n",
    "            \\b((?:\\d{1,2}/){2}(?:\\d{4}|\\d{2}))\\b     # dates, e.g. 5/12/2009\n",
    "          | \\ ((?:[A-Z][\\w\\-\\.]+\\ *)*(?:[A-Z]+[\\w\\-\\.]+)) # proper names, e.g. United Nations or Jean-Paul Sartre,\n",
    "          # when using the verbose flag, we can no longer use ' ' to match a space character, hence the use of '\\ '\n",
    "          | ([\\$€]\\ ?\\d+(?:[\\.,]\\d+)?)  # monetary amounts, e.g. $12.40\n",
    "\"\"\"\n",
    "\n",
    "tokenized2 = nltk.regexp_tokenize(raw2, pattern2)\n",
    "tokenized2 = [token for tokens in tokenized2 for token in tokens if token]\n",
    "tokenized2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting with no argument:\n",
      "['Back', 'in', 'the', '90s,', 'I', 'was', 'in', 'a', 'very', 'famous', 'TV', 'show.']\n",
      "Splitting with one space argument:\n",
      "['Back', 'in', 'the', '90s,\\tI', 'was', 'in', 'a', 'very', 'famous\\n\\n', 'TV', 'show.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Back in the 90s,\\tI was in a very famous\\n\\n TV show.\"\n",
    "\n",
    "print(\"Splitting with no argument:\")\n",
    "print(sent.split())\n",
    "print(\"Splitting with one space argument:\")\n",
    "print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.split()` sin argumentos siempre separa la cadena en los espacios, las tabulaciones y los saltos de línea. Cualquier secuencia de éstos, siempre dará lugar a una única separación.\n",
    "Si le pasamos un argumento, `.split()` utilizará un algoritmo diferente: separará en cada una de las ocurencias de este argumento. Por ejemplo, si el argumento es un espacio, hará un separación cada vez que encuentre un espacio, y no considerará las tabulaciones y saltos de línea como espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'Ziggy played guitar, jamming good with Weird and Gilly and the spiders from Mars.'.split()\n",
    "sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ziggy', 'played', 'guitar,', 'jamming', 'good', 'with', 'Weird', 'and', 'Gilly', 'and', 'the', 'spiders', 'from', 'Mars.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gilly', 'Mars.', 'Weird', 'Ziggy', 'and', 'and', 'from', 'good', 'guitar,', 'jamming', 'played', 'spiders', 'the', 'with']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sorted(my_list)` nos devuelve una nueva lista ordenada, mientras que la lista original sigue igual.  \n",
    "\n",
    "`my_list.sort()` cambia la lista original, ordenándola, pero no la devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    py\n",
      "py    thon\n"
     ]
    }
   ],
   "source": [
    "print(\"{:>6}\".format(\"py\"))\n",
    "print(\"{:6}{}\".format(\"py\", \"thon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos el nuevo método de formatear las cadenas, tenemos que sustituir %6s y %-6s por :>6 y :6 respectivamente.   \n",
    "\n",
    "Lo que hacen es imprimir x espacios antes o después de la cadena, respectivamente, donde x corresponde a `6 menos la longitud de la cadena`. En nuestro caso, la longitud de `py` es 2, por lo que se imprimirán 4 espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Why\", 'What', 'what', 'when', 'When', 'Where', 'where', 'which', 'Which', 'WHO', 'who', 'Who', 'whom', 'whose', 'Why', 'why']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_token(token):\n",
    "    \"\"\"lowercases the token and removes non-alpha chars from it.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z]', '', token).lower()\n",
    "\n",
    "def get_wh_tokens(tokens):\n",
    "    \"\"\"returns a list of all the wh_words contained in tokens, \n",
    "    preserving their original spelling and including non-alpha chars they may contain\"\"\"\n",
    "    wh_words = (\"what\", \"where\", \"when\", \"why\", \"who\", \"which\", \"whose\", \"whom\")\n",
    "    wh_tokens = [token for token in tokens \n",
    "                if normalize_token(token) in wh_words]\n",
    "    wh_tokens = sorted(set(wh_tokens), key=lambda s: s.lower())\n",
    "    return wh_tokens\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "wh_tokens = get_wh_tokens(tokens)\n",
    "\n",
    "wh_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['random', 10], ['words', 44], ['collection', 90], ['for', 75], ['the', 89], ['exercise', 36], ['number', 19], ['nineteen', 34], ['from', 6], ['chapter', 2], ['two', 906]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/2_19.txt') as f:\n",
    "    raw = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "freqs = [[word.split()[0], int(word.split()[1])] for word in raw]\n",
    "    \n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11°C\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.aemet.es/es/eltiempo/prediccion/municipios/madrid-id28079\"\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "divs = soup.findAll(\"div\", { 'class' : 'no_wrap'})\n",
    "\n",
    "for div in divs:\n",
    "    if div.get_text()[-2:] == '°C': # 1st \"no_wrap\" div with content ending in '°C' corresponds to the current temp.\n",
    "        print (div.get_text())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accessable', 'admitting', 'ages', 'albert', 'alternatives', 'angles', 'approached', 'articles', 'ashtray', 'asked', 'asks', 'assembled', 'awited', 'began', 'benefited', 'benefits', 'blog', 'blogger', 'bohr', 'bookmarked', 'box', 'boyfriend', 'brands', 'brewed', 'carefull', 'causes', 'christine', 'cigarettes', 'cigerette', 'clicked', 'co', 'com', 'commenting', 'comments', 'companies', 'complexions', 'concluded', 'controlled', 'cooked', 'cookies', 'corners', 'coughing', 'coworkers', 'credits', 'deaths', 'delivered', 'dictates', 'didn', 'directions', 'disposables', 'doesn', 'domingue', 'draining', 'drawings', 'ecigarettedirect', 'edges', 'email', 'emblazoned', 'embraced', 'endif', 'enthousiastic', 'etc', 'events', 'exhumed', 'extatic', 'eyes', 'feet', 'fields', 'fights', 'filed', 'flavors', 'flavoured', 'flitted', 'follows', 'followup', 'friends', 'gic', 'governments', 'groceries', 'grows', 'guides', 'hands', 'happened', 'happens', 'has', 'hast', 'healthier', 'heated', 'helped', 'html', 'https', 'humongously', 'hundreds', 'identified', 'implements', 'improved', 'including', 'infections', 'inhaling', 'introduced', 'invented', 'inventing', 'inventions', 'invited', 'items', 'joined', 'kidding', 'kuemper', 'legislators', 'lighted', 'lives', 'll', 'looks', 'loved', 'lungs', 'makes', 'markets', 'medicated', 'medications', 'members', 'menthols', 'mentioned', 'michael', 'months', 'moves', 'mr', 'needed', 'nestled', 'notes', 'offered', 'offers', 'oozing', 'organisations', 'others', 'packaging', 'packing', 'packs', 'paid', 'parents', 'passed', 'paul', 'photos', 'pictures', 'pillows', 'pioneered', 'placed', 'places', 'pm', 'posts', 'powers', 'predicted', 'proceeded', 'produces', 'producing', 'products', 'promoting', 'protected', 'prototypes', 'proud', 'provides', 'published', 'purchasing', 'pursuing', 'quacks', 'reached', 'reared', 'recommendations', 'recruited', 'referring', 'regards', 'relatives', 'reported', 'revamped', 'rewards', 'rights', 'ripped', 'roadmap', 'roamed', 'saviour', 'says', 'screamed', 'searched', 'shakedly', 'shared', 'shouted', 'showed', 'smokers', 'solved', 'staples', 'started', 'starts', 'stored', 'styrofoam', 'supplies', 'swims', 'taxes', 'teenager', 'tells', 'texting', 'things', 'thoughts', 'tis', 'tutorials', 'types', 'uk', 'unforgivig', 'updated', 'updating', 'using', 'vape', 'vaper', 'vapers', 'vaping', 'vapour', 've', 'ventured', 'versions', 'viewing', 'views', 'voices', 'walks', 'wanted', 'wants', 'warnings', 'wasn', 'words', 'www', 'years', 'yielded']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This solution uses the text_from_web function from exercise 8.\n",
    "\n",
    "def unknown(url):\n",
    "    raw = text_from_web(url)\n",
    "    words = re.findall(r'\\b[a-z]+\\b', raw)\n",
    "    words = sorted(set(words))\n",
    "    wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "    unknown_words = [word for word in words if word not in wordlist]\n",
    "    return unknown_words\n",
    "\n",
    "unknown('https://www.ecigarettedirect.co.uk/ashtray-blog/2013/10/interview-inventor-e-cigarette-herbert-a-gilbert.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista de palabras proporcionada por `nltk.corpus.words` siendo bastante incompleta, la mayoría de las palabras \"desconocidas\" son en realidad palabras totalmente válidas y comunes ('credits', 'box', 'looks', etc). Solo en algunos casos se trata de palabras mal escritas ('cigerette'), de trozos de html que BeautifulSoup no ha logrado eliminar ('endif'), de nombres propios escritos en minúsculas ('uk') o de partes de contracciones ('didn')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'smoke', '.', '¿', '¡', 'Do', \"n't\", 'you', 'know', '?', '!']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I don't smoke.     ¿¡Don't you know?!\"\n",
    "matches = re.findall(r\"([Dd]o)(n't)|(\\w+)|(\\S)\", sent)\n",
    "\n",
    "matches = [token for match in matches for token in match if token]\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión `«n't|\\w+»` no funciona. Si la aplicamos a \"don't\", tanto \"n't\" como \"don\" son resultados válidos. Sin embargo, los elementos que devuelve `findall` nunca se solapan, entonces solo se devolverá uno de ello. Después de haberlo probado con más cadenas, he llegado a la conclusión de que se prioriza el elemento más cercano al principio de la cadena. En este caso, es \"don\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pzzth0n 1s an 1nt3rpr3t3stu,  7 0bj3ct-0r13nt3stu, h1gh-|3v3| pr0gramm1ng |anguag3 w1th stuzznam1c $3mant1cs5w33t! \\n1ts h1gh-|3v3| bu1|t 1n stuata $tructur3s, c0mb1n3stu w1th stuzznam1c tzzp1ng anstu stuzznam1c b1nstu1ng, \\nmak3 1t v3rzz attract1v3 f0r rap1stu app|1cat10n stu3v3|0pm3nt, as w3|| as f0r u53 as a $cr1pt1ng \\n0r g|u3 |anguag3 t0 c0nn3ct 3x15t1ng c0mp0n3nts t0g3th3r5w33t! 1 at3 a $k1nnzz pzzth0n5w33t!'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiple_replace(text, dic):\n",
    "    for key, value in dic.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "    \n",
    "text = \"\"\"Python is an interpreted,  7 object-oriented, high-level programming language with dynamic semantics. \n",
    "Its high-level built in data structures, combined with dynamic typing and dynamic binding, \n",
    "make it very attractive for Rapid Application Development, as well as for use as a scripting \n",
    "or glue language to connect existing components together. I ate a skinny python.\"\"\"\n",
    "\n",
    "dic = {\n",
    "    r'e': r'3',\n",
    "    r'i': r'1',\n",
    "    r'o': r'0',\n",
    "    r'l': r'|',\n",
    "    r'\\bs': r'$',\n",
    "    r'(?P<start>\\w+)s(?P<end>\\w+)': r'\\g<start>5\\g<end>', #word-end s is not considered\n",
    "    r'\\.': r'5w33t!',\n",
    "    r'ate': r'8',\n",
    "    r'd': r'stu',\n",
    "    r'y': r'zz'\n",
    "}\n",
    "\n",
    "multiple_replace(text.lower(), dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. http://en.wikipedia.org/wiki/Pig_Latin\n",
    "\n",
    "    Write a function to convert a word to Pig Latin.\n",
    "    Write code that converts text, instead of individual words.\n",
    "    Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting words to Pig Latin:\n",
      "look --> ooklay\n",
      "stamp --> ampstay\n",
      "GRound --> oundGRay\n",
      "Cherry --> ErryChay\n",
      "idle --> idleay\n",
      "Quick --> IckQuay\n",
      "squeeze --> eezesquay\n",
      "yellow --> ellowyay\n",
      "style --> ylestay\n",
      "You --> OuYay\n",
      "SYD --> YDSay\n",
      "oe --> oeay\n",
      "\n",
      "\n",
      "Converting a sentence into Pig Latin:\n",
      "This is quite a long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\n",
      "------->\n",
      "IsThay isay itequay aay onglay \"entencesay\" 5+2... EsYay, itay isay!!! ¿Arenay'tay erethay otway entencessay erehay?\n"
     ]
    }
   ],
   "source": [
    "def word_to_pig(word):\n",
    "    if not word.isalpha():\n",
    "        return word\n",
    "    pattern = re.compile(r'(?P<start>^([yY])?[^aeioAEIO(qu)(Qu)(QU)(qU)uyUY]*([qQ][uU])?)(?P<end>\\w*)')\n",
    "    pig_word = re.sub(pattern, \n",
    "                      r'\\g<end>\\g<start>ay', \n",
    "                      word)\n",
    "    if word.istitle():\n",
    "        pig_word = pig_word[0].title() + pig_word[1:]\n",
    "    return pig_word\n",
    "\n",
    "def tokenize_text(raw):\n",
    "    tokens= re.findall(r\"[a-zA-Z]+|\\S|\\s\", raw)\n",
    "    return tokens\n",
    "\n",
    "def sent_to_pig(sent):\n",
    "    tokens = tokenize_text(sent)\n",
    "    pig_tokens = [word_to_pig(token)  \n",
    "                  for token in tokens]\n",
    "    pig_sent = ''.join(pig_tokens)\n",
    "    return pig_sent\n",
    "\n",
    "\n",
    "words = ['look', 'stamp', 'GRound', 'Cherry', 'idle', 'Quick', \n",
    "         'squeeze', 'yellow', 'style', 'You', 'SYD', 'oe']\n",
    "sent = \"\"\"This is quite a long \"sentence\" 5+2... Yes, it is!!! ¿Aren't there two sentences here?\"\"\"\n",
    "\n",
    "print('Converting words to Pig Latin:')\n",
    "for word in words:\n",
    "    print('{} --> {}'.format(word, word_to_pig(word)))\n",
    "\n",
    "print('\\n')\n",
    "print(\"Converting a sentence into Pig Latin:\")\n",
    "print(sent)\n",
    "print('------->')\n",
    "print(sent_to_pig(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 26."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a e i u á é \n",
      "a 0 0 1 1 0 0 \n",
      "e 1 1 1 0 0 1 \n",
      "i 1 1 1 0 1 0 \n",
      "á 0 0 0 0 0 1 \n",
      "ó 1 0 0 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "raw = ' '.join(nltk.corpus.udhr.words('Hungarian_Magyar-Latin1'))\n",
    "vowel_seqs = set(re.findall(r'[aeiouAEIOUáéíóöőúüűÁÉÍÓÖŐÚÜŰ]{2,}', raw))\n",
    "vowel_seqs = [tuple(seq) for seq in vowel_seqs]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(vowel_seqs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hea heaaheaa ahahaa ahahhh hahahh ehe ahehhah hhhha a a a hh heeahaehhh hehe haheh hhahea hah hh a hae hhaee ahhh ea h e hh hh hhhahahaahhahahheeeheahehh a eaah hah e heahhhehh ha h e hahaha he eahahhhhaa hhhhhaa aahh a hhhe hhaah ee aaa hhe ehh e eehehheeeeahheeea hehheeehhha eh aeaeahehhh hehhhh hhae hhaahhhheahhhheehahahhaaeaeeaea a ahaheehaheaehaehea hhehh a haee aahhh eheh eha h hehheeehhhhhhahhehe ehh hae h ahhhhh hh heaahahhhh h hhhha h eaahhaaeaah aeeae hhah haheheeh ahhe\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "chars = 'aehh '\n",
    "text = ''.join([choice(chars) for i in range(0,500)])\n",
    "text = ' '.join(text.split())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the adventure section of the Brown corpus, the ARI index equals to 4.08.\n",
      "For the belles_lettres section of the Brown corpus, the ARI index equals to 10.99.\n",
      "For the editorial section of the Brown corpus, the ARI index equals to 9.47.\n",
      "For the fiction section of the Brown corpus, the ARI index equals to 4.91.\n",
      "For the government section of the Brown corpus, the ARI index equals to 12.08.\n",
      "For the hobbies section of the Brown corpus, the ARI index equals to 8.92.\n",
      "For the humor section of the Brown corpus, the ARI index equals to 7.89.\n",
      "For the learned section of the Brown corpus, the ARI index equals to 11.93.\n",
      "For the lore section of the Brown corpus, the ARI index equals to 10.25.\n",
      "For the mystery section of the Brown corpus, the ARI index equals to 3.83.\n",
      "For the news section of the Brown corpus, the ARI index equals to 10.18.\n",
      "For the religion section of the Brown corpus, the ARI index equals to 10.2.\n",
      "For the reviews section of the Brown corpus, the ARI index equals to 10.77.\n",
      "For the romance section of the Brown corpus, the ARI index equals to 4.35.\n",
      "For the science_fiction section of the Brown corpus, the ARI index equals to 4.98.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "def compute_ari(category):\n",
    "    \"\"\"\n",
    "    Computes the average readability of a text based on\n",
    "    average word length and average sentences length.\n",
    "    \"\"\"    \n",
    "    muw, mus = get_muw_mus(category)\n",
    "    ari = 4.71 * muw + 0.5 * mus - 21.43\n",
    "    return ari\n",
    "\n",
    "def compute_average_len(elements):\n",
    "    \"\"\"\n",
    "    Compute the average length of the elements of a list.\n",
    "    Can be used both for a list of lists and for a list of strings.\n",
    "    \"\"\"\n",
    "    sum_lens = sum([len(elm) for elm in elements])\n",
    "    return sum_lens / len(elements)\n",
    "\n",
    "def get_muw_mus(category):\n",
    "    \"\"\"\n",
    "    Returns the average lengths of the words (muw) \n",
    "    and the sentences (mus) for a specific category of the Brown corpus.\n",
    "    \"\"\"\n",
    "    muw = compute_average_len(brown.words(categories=category))\n",
    "    mus = compute_average_len(brown.sents(categories=category))\n",
    "    return (muw, mus)\n",
    "\n",
    "for category in brown.categories():\n",
    "    ari = compute_ari(category)\n",
    "    print('For the {} section of the Brown corpus, the ARI index equals to {}.'.format(category, round(ari, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "['the', 'click', 'OF', 'cuthbert', '***', 'produc', 'by', 'suzann', 'L.', 'shell', ',', 'charl', 'frank', 'and', 'the', 'onlin', 'distribut', 'proofread', 'team', 'the', 'click', 'OF', 'cuthbert', 'by', 'P.', 'G.', 'wodehous', '1922', 'dedic', 'TO', 'the', 'immort', 'memori', 'OF', 'john', 'henri', 'and', 'pat', 'rogi', 'who', 'AT', 'edinburgh', 'IN', 'the', 'year', '1593', 'a.d', '.', 'were', 'imprison']\n",
      "Lancaster Stemmer:\n",
      "['the', 'click', 'of', 'cuthbert', '***', 'produc', 'by', 'suzan', 'l.', 'shel', ',', 'charl', 'frank', 'and', 'the', 'onlin', 'distribut', 'proofread', 'team', 'the', 'click', 'of', 'cuthbert', 'by', 'p.', 'g.', 'wodeh', '1922', 'ded', 'to', 'the', 'immort', 'mem', 'of', 'john', 'henry', 'and', 'pat', 'rog', 'who', 'at', 'edinburgh', 'in', 'the', 'year', '1593', 'a.d', '.', 'wer', 'imprison']\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/7028/pg7028.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "first = raw.find(\"THE CLICKING OF CUTHBERT\")\n",
    "last = raw.rfind(\"End of Project Gutenberg's The Clicking of Cuthbert\")\n",
    "raw = raw[first:last]\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "stemmed_porter = [porter.stem(token) for token in tokens]\n",
    "stemmed_lancaster = [lancaster.stem(token) for token in tokens]\n",
    "\n",
    "print('Porter Stemmer:')\n",
    "print(stemmed_porter[:50])\n",
    "print('Lancaster Stemmer:')\n",
    "print(stemmed_lancaster[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El stemmer de Lancaster lo pasa todo a minúsculas, mientras que el Porter no lo hace. También el Lancaster considera como terminación todo lo que en teoría podría serlo. Por ejemplo, \"Wodehouse\" -> \"Wodeh\", \"memory\" -> \"mem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "\n",
    "lengths = []\n",
    "for word in saying:\n",
    "    lengths.append(len(word))\n",
    "    \n",
    "lengths2 = [len(word) for word in saying]\n",
    "\n",
    "print(lengths2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating\n",
    "way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "\n",
    "    Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.\n",
    "    Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.\n",
    "    Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.\n",
    "    Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = silly.split()\n",
    "second_chars = ''.join([word[1] for word in bland if len(word) >= 2])\n",
    "new_silly = ' '.join(bland)\n",
    "\n",
    "for word in sorted(bland):\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "\n",
    "    What happens when you look up a substring, e.g. 'inexpressible'.index('re')?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando buscamos una subcadena con `index`, se devuelve la posición del primer caracter de la subcadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.\n",
    "    Define a variable silly as in the exercise above. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newly formed bland ideas are inexpressible\n"
     ]
    }
   ],
   "source": [
    "words = ['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'said', 'than', 'done']\n",
    "said_index = words.index('said')\n",
    "\n",
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "silly_split = silly.split()\n",
    "i = silly_split.index('in')\n",
    "silly_sliced = ' '.join(silly_split[:i])\n",
    "\n",
    "print(silly_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
