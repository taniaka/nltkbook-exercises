{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK BOOK. Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union, brown, stopwords, gutenberg, wordnet as wn, nps_chat, udhr, names\n",
    "from nltk import Text, FreqDist, ConditionalFreqDist, ConcordanceIndex\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos unas variables que se utilizarán en varios ejercicios del capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = \". , : ; ! ? - < > ' ''  ` `` -- .' ,' ?' !' * --'\".split() +\\\n",
    "',\" .\" ?\" !\" \"'.split()\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "stops_punct = stops + punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the texts of the State of the Union addresses, using the state_union corpus reader. Count occurrences of men, women, and people in each document. What has happened to the usage of these words over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su_cfd = ConditionalFreqDist(\n",
    "          (fileid[:4], target)\n",
    "          for fileid in state_union.fileids()\n",
    "          for word in state_union.words(fileid)\n",
    "          for target in ['men', 'women', 'people']          \n",
    "          if word.lower()== target          \n",
    ")\n",
    "su_cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"People\" siempre ha sido más usado que \"men\" and \"women\".  \n",
    "\n",
    "Antes de los finales de los 70, \"men\" era un poco más frecuente que \"women\", ya que en algunas ocasiones se utilizaba en lugar de \"people\".Sobre todo esto se nota en los discursos del presidente Johnson de los años 1965-1967.  \n",
    "\n",
    "A partir del 1978 y salvo alguna excepción, \"women\" es más usado que \"men\", ya que se menciona en el contexto de discriminación.   \n",
    "\n",
    "El pequeño repunte de \"men\" and \"women\" (en comparación con el uso de \"people\") en los años 2004-2006, durante el mandato de George W. Bush, podría tener que ver tanto con las preferencias lingüísticas del presidente como con la guerra en Irak, ya que en varias ocasiones \"our men and women\" se usa para referirse a los soldados luchando en Irak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_synsets = [wn.synset('nose.n.01'), wn.synset('telephone.n.01'), wn.synset('tree.n.01'), wn.synset('water.n.01')]\n",
    "\n",
    "for synset in my_synsets:\n",
    "    print(synset.name(), '\\t', synset.definition())\n",
    "    print('is member of {} '.format(synset.member_holonyms()))\n",
    "    print('its members are {}'.format(synset.member_meronyms()))\n",
    "    print('is part of {} '.format(synset.part_holonyms()))\n",
    "    print('its parts are {}'.format(synset.part_meronyms()))\n",
    "    print('is as a substance used in {} '.format(synset.substance_holonyms()))\n",
    "    print('is made of the following substances {}'.format(synset.substance_meronyms()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Strunk and White's Elements of Style, the word however, used at the start of a sentence, means \"in whatever way\" or \"to whatever extent\", and not \"nevertheless\". They give this example of correct usage: However you advise him, he will probably do as he thinks best. (http://www.bartleby.com/141/strunk3.html) Use the concordance tool to study actual usage of this word in the various texts we have been considering. See also the LanguageLog posting \"Fossilized prejudices about 'however'\" at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in brown.categories():\n",
    "    text = brown.words(categories=category)\n",
    "    print(\"Category: {}\".format(category))\n",
    "    print (ConcordanceIndex(text).print_concordance(\"However\", width=200, lines=100))\n",
    "    print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método \"concordance\" de nltk es case-insensitive, por eso utilizo la clase ConcordanceIndex que sí permite sacar las concordancias solo para las palabras que empiezan en mayúsculas y así, es decir solo para los casos en los que \"however\" está al principio de una oración.  \n",
    "\n",
    "De los más de 170 casos de uso de \"however\" al inicio de una oración en el Brown Corpus, solo 8, bajo mi opinión, podrían ser sustituidos por \"to whatever extent\", lo que nos lleva a pensar que se trata de un uso no muy frecuente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a conditional frequency distribution over the Names corpus that allows you to see which initial letters are more frequent for males vs. females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in names.fileids():\n",
    "    print(\"There are {} {} names\".format(len(names.words(fileid)), fileid[:-4]))\n",
    "\n",
    "gender_cfd = ConditionalFreqDist(\n",
    "          (fileid[:-4], first_name[0])\n",
    "          for fileid in names.fileids()\n",
    "          for first_name in names.words(fileid)              \n",
    ")\n",
    "gender_cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay más nombres masculinos que femeninos en el corpus, por lo que si quisieramos comparar las frecuencias deberíamos primero haberlas normalizado. Sin embargo, a simple vista ya se puede ver que el porcentaje de nombres en \"H\", \"O\", \"Q\", \"T\", \"U\", \"W\", \"X\", \"Y\", \"Z\" entre los nombres masculinos es más elevado que entre los femeninos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a pair of texts and study the differences between them, in terms of vocabulary, vocabulary richness, genre, etc. Can you find pairs of words which have quite different meanings across the two texts, such as monstrous in Moby Dick and in Sense and Sensibility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['chesterton-thursday.txt', 'carroll-alice.txt']\n",
    "texts = [Text(gutenberg.words(file), file) for file in files]\n",
    "words = \"rich poor\".split()\n",
    "row_format = '{:<25} {:<10} {:<10} {:<10} {:<10}'\n",
    "\n",
    "def text_richness(file):\n",
    "    tokens = len(text)\n",
    "    types = len(set(text))\n",
    "    lexical_diversity = round(types/tokens, 2)\n",
    "    stops_num = 0\n",
    "    for word in text:\n",
    "        if word in stops:\n",
    "            stops_num += 1\n",
    "    stops_part = round(stops_num/tokens, 2)\n",
    "    return(tokens, types, lexical_diversity, stops_part)\n",
    "\n",
    "def get_dist(text, stops=None):\n",
    "    text = [w.lower() for w in text]\n",
    "    if stops:\n",
    "        text = [w for w in text if w not in stops_punct]\n",
    "    fdist = FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def get_collocations(text):\n",
    "    return text.collocations(35)\n",
    "\n",
    "def get_concordance(word, text):\n",
    "    return text.concordance(word)\n",
    "    \n",
    "    \n",
    "print(row_format.format('', 'tokens', 'types', 'diversity', 'stopwords'))\n",
    "\n",
    "for text in texts:\n",
    "    tokens, types, lexical_diversity, stops_part = text_richness(text)\n",
    "    print(row_format.format(text.name, tokens, types, lexical_diversity, stops_part))\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "for text in texts:\n",
    "    print(\"Collocations: \", text.name)\n",
    "    print(get_collocations(text))\n",
    "    print ('\\n\\n')\n",
    "\n",
    "for text in texts:\n",
    "    print(get_dist(text, stops + punctuation).most_common(50))\n",
    "    print ('\\n\\n')\n",
    "    \n",
    "for word in words:\n",
    "    for text in texts:\n",
    "        print(word, ': ', text.name)\n",
    "        print(get_concordance(word, text))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He comparado \"The Man Who Was Thursday\" de Chesterton con \"Alice in Wonderland\" de Carrol.\n",
    "\n",
    "Los textos tienen unos índices de diversidad léxica casi idénticos (0,1 y 0,09) y la parte del texto ocupada por stopwords también es casi la misma (0.39 y 0.38).\n",
    "\n",
    "De las colocaciones de \"The Man Who Was Thursday\" se desprende que se podría tratar de una novela policíaca (\"Inspector Ratcliffe\", \"Scotland Yard\", \"police station\"). Y basándonos en las colocaciones de \"Alice\", podemos deducir que muchos de los personajes son animales (\"Mock Turtle\", \"March Hare\", etc.), por lo que debería tratarse de un cuento fantástico. También algunas colocaciones parecen indicar que en el libro hay mucho diálogo (\"said Alice\", \"trembling voice\", \"dead silence\", \"yer honor\", \"Alice replied\").\n",
    "\n",
    "Finalmente he mirado las concordancias de \"rich\" and \"poor\" en los dos textos. En el texto de Chesterton ambas palabras se usan tanto en relación con la situación económica de una persona o entidad (\"rich man\", \"the poor have been rebels\"), como en el sentido figurativo de \"abundante\" o \"suntuoso\" para \"rich\" (\"rich athmosphere\") y de desgraciado para \"poor\" (\"poor old Colonel\"). En el libro de Carrol la riqueza material, al parecer, no tiene mucha transcendencia. \"Rich\" se usa solo una vez para hablar de la sopa, mientras que \"poor\" se utiliza 27 veces y casi siempre para expresar compasión (\"poor Alice\", \"poor little things\", \"my poor hands\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the BBC News article: UK's Vicky Pollards 'left behind' http://news.bbc.co.uk/1/hi/education/6173441.stm. The article gives the following statistic about teen language: \"the top 20 words used, including yeah, no, but and like, account for around a third of all words.\" How many word types account for a third of all word tokens, for a variety of text sources? What do you conclude about this statistic? Read more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n",
    "'science_fiction']\n",
    "\n",
    "\n",
    "def types_in_part(fdist, n):\n",
    "    # Set the number of tokens corresponding to n% of all the tokens\n",
    "    mark = fdist.N() * n\n",
    "    tokens = 0\n",
    "    for count, (word, freq) in enumerate(fdist.most_common()):\n",
    "        tokens += freq\n",
    "        if tokens >= mark:\n",
    "            types_number = count + 1\n",
    "            break\n",
    "    return types_number\n",
    "\n",
    "brown_cfdist = ConditionalFreqDist(\n",
    "        (category, word.lower())\n",
    "        for category in categories\n",
    "        for word in brown.words(categories=category)\n",
    "        if word.lower() not in stops_punct\n",
    "    )\n",
    "\n",
    "for fdist in brown_cfdist:\n",
    "    length = len(brown_cfdist[fdist])\n",
    "    types = types_in_part(brown_cfdist[fdist], 0.33)\n",
    "    print(\"Category: {}. \\nNumber of types in category: {}. \"\n",
    "          \"\\nWord types accounting for 1/3 of tokens: {}.\"\n",
    "          \"\\n% of types accounting for 1/3 of tokens: {}.\\n\"\n",
    "          .format(fdist, length, types, round(types/length,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo que para poder comparar estos números deberíamos ver el porcentaje y no el número absoluto de los types que forman 1/3 de los tokens.  \n",
    "\n",
    "Parece que \"science fiction\", \"humor\" y \"reviews\" son las categorías en las que las palabras más comunes ocupan un menor porcentaje del número total de tokens. Por el momento, no se me ocurre ninguna explicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sets = list(wn.all_synsets('n'))\n",
    "hypless = 0\n",
    "\n",
    "for synset in all_sets:\n",
    "    if not len(synset.hyponyms()):\n",
    "        hypless += 1\n",
    "   \n",
    "print('{} percent of nouns have no hyponyms.'.format(round(hypless/len(all_sets)*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supergloss(s):\n",
    "    data = []\n",
    "    data.append(s.definition())\n",
    "    for nyms in [s.hypernyms(), s.hyponyms()]:\n",
    "        for synset in nyms:\n",
    "            data.append(synset.definition())\n",
    "    return '\\n '.join(data)\n",
    "\n",
    "print(supergloss(wn.synset('table.n.01')))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to find all words that occur at least three times in the Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_occurence(text, min_freq):\n",
    "    text = [w.lower() for w in text]\n",
    "    fdist = FreqDist(text)\n",
    "    common_words = sorted([w for w in set(text) if fdist[w] >= min_freq])\n",
    "    return common_words\n",
    "\n",
    "brown_3 = word_occurence(brown.words(), 3)\n",
    "print(\"There are {} words that occur at least 3 times in the Brown corpus.\".format(len(brown_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in 1.1. Include the full set of Brown Corpus genres (nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest number of tokens per type)? Is this what you would have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = nltk.corpus.brown.categories()\n",
    "for category in brown.categories():\n",
    "    words = brown.words(categories=category)\n",
    "#     words = [word.lower() for word in words]\n",
    "    lexical_diversity =  len(set(words)) / len(words)\n",
    "    print(category, round(lexical_diversity, 3), len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tres categorías con menor diversidad léxica son las de \"learned\", \"belles lettres\" y \"government\". Para \"government\" esto se podría explicar por el hecho de que se trata de un área muy específica que excluye casi por completo el uso de vocabulario propio del lenguaje coloquial, periodístico, etc. En el caso de \"government\" y \"learned\" la única explicación que se me ocurre es que los dos son de mayor extensión que el resto de los subcorpous de Brown (un texto de 10000 tendría una menor diversidad léxica que uno de 100). De este mismo modo podríamos explicar la diversidad en las categorías de \"science fiction\", \"reviews\" y \"religion\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that finds the 50 most frequently occurring words of a text that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_50(text):\n",
    "    text = [word for word in text if word.lower() not in stops]\n",
    "    fdist = FreqDist(text)\n",
    "    return fdist.most_common(50)\n",
    "\n",
    "most_frequent_50(brown.words(categories='fiction'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_bigrams(sents, num):\n",
    "    bigrams = []\n",
    "    for sent in sents:\n",
    "        bigrams += list(nltk.bigrams(sent))\n",
    "    bigrams = [(w_1, w_2) for (w_1, w_2) in bigrams \n",
    "               if w_1.lower() not in stops_punct \n",
    "               and w_2.lower() not in stops_punct]\n",
    "    fdist = FreqDist(bigrams)\n",
    "    return fdist.most_common(num)\n",
    "\n",
    "most_frequent_bigrams(brown.sents(categories='fiction'), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de los stopwords he omitido también los signos de puntuación.  \n",
    "\n",
    "En vez de pasarle una lista de palabras a la función le paso una lista de oraciones, así se evita que se considere como bigram la combinación de la última y primera palabras de las frases adyacentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to create a table of word frequencies by genre, like the one given in 1 for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = [\"can't\", \"cannot\"]\n",
    "categories = brown.categories()\n",
    "\n",
    "cfd = ConditionalFreqDist(\n",
    "    (category, target)\n",
    "    for category in categories\n",
    "    for word in brown.words(categories=category)\n",
    "    for target in my_words\n",
    "    if word.lower() == target\n",
    ")\n",
    "\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos comparado el uso de \"can't\" y \"cannot\" en los texto de distintas categorías. Como era de esperar, \"can't\" es más presente que \"cannot\" en las cateogrías que corresponde a las novelas, ya que en éstas se imita mucho el lenguaje coloquial. En las noticias \"can't\" se usa con la misma frecuencia que \"cannot\", lo que se podría atribuir a la abundancia de citas directas en las noticias. En las demás categorías vemos una mayor frecuencia de \"cannot\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function word_freq() that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(word, section):\n",
    "    text = brown.words(categories=section)\n",
    "    text = [w.lower() for w in text]\n",
    "    fdist = FreqDist(text)\n",
    "    return fdist[word]\n",
    "\n",
    "word_freq('my', 'fiction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function hedge(text) which processes a text and produces a new version with the word 'like' between every third word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedge(text):\n",
    "    new_text = []\n",
    "    order = 0\n",
    "    for word in text:\n",
    "        new_text.append(word)\n",
    "        order += 1\n",
    "        if order == 3:\n",
    "            if word not in punctuation:\n",
    "                new_text.append('like')\n",
    "                order = 0\n",
    "            else:\n",
    "                order = 2\n",
    "    return new_text\n",
    "\n",
    "hedge(brown.words(categories='fiction'))[:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let f(w) be the frequency of a word w in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf's law states that the frequency of a word type is inversely proportional to its rank (i.e. f × r = k, for some constant k). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Write a function to process a large text and plot word frequency against word rank using pylab.plot. Do you confirm Zipf's law? (Hint: it helps to use a logarithmic scale). What is going on at the extreme ends of the plotted line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank_freq(text):\n",
    "    text = [w.lower() for w in text]\n",
    "    fdist = FreqDist(text)\n",
    "    word_freq = fdist.most_common()\n",
    "    ranks, freqs = list(), list()\n",
    "    for count, tupla in enumerate(word_freq):\n",
    "        ranks.append(count+1)\n",
    "        freqs.append(tupla[1])\n",
    "    pyplot.plot(ranks, freqs)\n",
    "    pyplot.title('Word rank against word frequency')\n",
    "    pyplot.xlabel('Rank')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.yscale('log')\n",
    "    pyplot.xscale('log')\n",
    "    pyplot.show()\n",
    "\n",
    "plot_rank_freq(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ley de Zipf no funciona en los extremos. En un texto normal, es prácticamente imposible que el primer type más común sea 2 veces más frecuente que el segundo más común. En el otro extremo, el type número 25000 y el número 50000 y todos los que están entre ellos y los siguen tendrán una frecuencia de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to include the space character. You will need to import random first. Use the string concatenation operator to accumulate characters into a (very) long string. Then tokenize this string, and generate the Zipf plot as before, and compare the two plots. What do you make of Zipf's Law in the light of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_string = \"abcdefg \"\n",
    "random_string = str()\n",
    "for i in range(2000000):\n",
    "    random_string += random.choice(alpha_string)\n",
    "random_words = random_string.split()\n",
    "\n",
    "plot_rank_freq(random_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este plot se desvía más aún de la ley de Zipf y no solo en los extremos. Creo que es debido a que las palabras de la misma longitud tienden a tener unas frecuencias muy similares. En una distribución sacada de un texto normal un type de dos letras puede darse 1 o 1000 veces, mientras que aquí eso no va a suceder. Por la misma razón, ciertas frecuencias simplemente nunca se van dar, de ahí este gráfico en \"escalones\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the text generation program in 2.2 further, to do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Store the n most likely words in a list words then randomly choose a word from the list using random.choice(). (You will need to import random first.)  \n",
    "\n",
    "\n",
    "b. Select a particular genre, such as a section of the Brown Corpus, or a genesis translation, one of the Gutenberg texts, or one of the Web texts. Train the model on this corpus and get it to generate random text. You may have to experiment with different start words. How intelligible is the text? Discuss the strengths and weaknesses of this method of generating random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, length, candidates_number):\n",
    "    for i in range(length):\n",
    "        print(word, end=' ')\n",
    "        candidates = [candidate for (candidate, frequency) in cfdist[word].most_common(candidates_number)]\n",
    "        word = random.choice(candidates)\n",
    "\n",
    "categories = ['fiction', 'news']\n",
    "candidates_nums = [3,6,10,15,40]\n",
    "words = ['the', 'orange']\n",
    "\n",
    "for category in categories:\n",
    "    bigrams = nltk.bigrams(brown.words(categories=category))\n",
    "    cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "    print(category)\n",
    "    print('\\n')\n",
    "    for word in words:\n",
    "        print('The word is {}'.format(word), '\\n')\n",
    "        for cand_num in candidates_nums:\n",
    "            print('Choosing from {} bigrams:'.format(cand_num), end=' ')\n",
    "            try:\n",
    "                generate_model(cfd, word, 40, cand_num)\n",
    "            except IndexError:\n",
    "                print(\"No such word.\")\n",
    "                break\n",
    "            else:\n",
    "                print('\\n')\n",
    "        print('\\n') \n",
    "    print('\\n')            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo que la \"inteligibilidad\" del texto generado no cambia dependiendo del número de bigrams de los que se elige al azar la siguiente palabra, lo que sí cambia, obviamente, es la diversidad de las palabras utilizadas. \n",
    "\n",
    "También cuanto menos frecuente la palabra por la que empezamos (por ejemplo, \"orange\"), más probable que generemos unos resultados parecidos, al menos en la primera parte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Exercise 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function find_language() that takes a string as its argument, and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [language for language in udhr.fileids() if 'Latin1' in language]\n",
    "\n",
    "def find_language(word):\n",
    "    languages_with_word = [language for language in languages\n",
    "                          if word.lower() in [w.lower() for w in udhr.words(language)]]\n",
    "    return languages_with_word\n",
    "\n",
    "find_language('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the branching factor of the noun hypernym hierarchy? I.e. for every noun synset that has hyponyms — or children in the hypernym hierarchy — how many do they have on average? You can get all noun synsets using wn.all_synsets('n')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sets = list(wn.all_synsets('n'))\n",
    "total_hyponyms = 0\n",
    "sets_with_hyponyms = 0\n",
    "\n",
    "for sset in all_sets:\n",
    "    if sset.hyponyms():\n",
    "        total_hyponyms += len(sset.hyponyms())\n",
    "        sets_with_hyponyms += 1\n",
    "average_hyponyms = total_hyponyms / sets_with_hyponyms\n",
    "round(average_hyponyms, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = ['n', 'v', 'a', 'r']\n",
    "\n",
    "for pos in poses:\n",
    "    all_sets = wn.all_synsets(pos)\n",
    "    words = [lemma.name() \n",
    "             for synset in list(all_sets) \n",
    "             for lemma in synset.lemmas()]\n",
    "    words = set(words)\n",
    "    synsets_number = 0\n",
    "    for word in words:\n",
    "        synsets_number += len(wn.synsets(word, pos))\n",
    "    average_synsets_number = synsets_number / len(words)\n",
    "    print(pos, round(average_synsets_number, 2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the predefined similarity measures to score the similarity of each of the following pairs of words. Rank the pairs in order of decreasing similarity. How close is your ranking to the order given here, an order that was established experimentally by (Miller & Charles, 1998): car-automobile, gem-jewel, journey-voyage, boy-lad, coast-shore, asylum-madhouse, magician-wizard, midday-noon, furnace-stove, food-fruit, bird-cock, bird-crane, tool-implement, brother-monk, lad-brother, crane-implement, journey-car, monk-oracle, cemetery-woodland, food-rooster, coast-hill, forest-graveyard, shore-woodland, monk-slave, coast-forest, lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [{'words': ('car', 'automobile')}, {'words': ('gem', 'jewel')}, {'words': ('journey', 'voyage')},\n",
    "         {'words': ('boy', 'lad')}, {'words': ('coast', 'shore')}, {'words': ('asylum', 'madhouse')}, \n",
    "         {'words': ('magician', 'wizard')}, {'words': ('midday', 'noon')}, {'words': ('furnace', 'stove')}, \n",
    "         {'words': ('food', 'fruit')}, {'words': ('bird', 'cock')}, {'words': ('bird', 'crane')}, \n",
    "         {'words': ('tool', 'implement')}, {'words': ('brother', 'monk')}, {'words': ('lad', 'brother')}, \n",
    "         {'words': ('crane', 'implement')}, {'words': ('journey', 'car')}, {'words': ('monk', 'oracle')}, \n",
    "         {'words': ('cemetery', 'woodland')}, {'words': ('food', 'rooster')}, {'words': ('coast', 'hill')}, \n",
    "         {'words': ('forest', 'graveyard')}, {'words': ('shore', 'woodland')}, {'words': ('monk', 'slave')},\n",
    "         {'words': ('coast', 'forest')}, {'words': ('lad', 'wizard')}, {'words': ('chord', 'smile')}, \n",
    "         {'words': ('glass', 'magician')}, {'words': ('rooster', 'voyage')}, {'words': ('noon', 'string')}]\n",
    "\n",
    "\n",
    "for pair in pairs:\n",
    "    # compare each word1 synset with each word2 synset\n",
    "    # and get the highest score\n",
    "    highest_score = 0\n",
    "    for synset1 in wn.synsets(pair['words'][0]):\n",
    "        for synset2 in wn.synsets(pair['words'][1]):\n",
    "            score = wn.path_similarity(synset1, synset2)\n",
    "            try:\n",
    "                if score > highest_score:\n",
    "                    highest_score = score\n",
    "            except TypeError:\n",
    "                continue\n",
    "    pair['score'] = round(highest_score, 2)\n",
    "\n",
    "sorted_by_score = sorted(pairs, key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "for elm in sorted_by_score:\n",
    "    print(\"Pair: {}. Score: {}\\n\".format(elm['words'], elm['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clasificación de Miller y Charles con los scores se puede ver por ejemplo [aquí](https://arxiv.org/ftp/arxiv/papers/1204/1204.0245.pdf).  \n",
    "\n",
    "El ranking generado por wordnet en general no se aparta mucho del de Miller y Charles, aunque hay excepciones: por ejemplo, para Miller y Charles la pareja 'lad-wizard' ocupa el quinto sitio desde el final, mientras que aquí está a la mitad del ranking con un score de 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos descubierto que el gráfico 1.2 del capítulo 2 no corresponde a la función que supuestamente lo genera. La función pretende \"to examine the differences in word lengths for a selection of languages included in the udhr corpus\" devuelve una distribución con valores absolutos, mientras en el gráfico del libro vemos unos porcentajes. Este es mi intento de modificar la función para poder generar un gráfico similar al del libro. Para simplificar las cosas solo he incluido dos lenguas en mi distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "\n",
    "languages = ['Chickasaw', 'English']\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "      (lang, len(word))\n",
    "      for lang in languages\n",
    "      for word in udhr.words(lang + '-Latin1'))\n",
    "\n",
    "rel_cfd = deepcopy(cfd) # Creamos una copia de la distribución condicional\n",
    "\n",
    "# Creamos copias de cada una de las distribuciones de frecuencias\n",
    "rel_fd1 = copy(cfd['Chickasaw'])\n",
    "rel_fd2 = copy(cfd['English'])\n",
    "\n",
    "def fdist_per(fd, i=100):\n",
    "    \"\"\"In a frequency distribution, replace absolute frequencies\n",
    "    by relative frequencies per every i items.\n",
    "    \"\"\"\n",
    "    length = fd.N()\n",
    "    for key, value in fd.items():\n",
    "        fd[key] = round(value / length * i, 3)\n",
    "    return fd\n",
    "\n",
    "#sustituimos las distribuciones con valores absolutos por distribuciones con valores relativos\n",
    "rel_cfd['Chickasaw'] = fdist_per(rel_fd1)\n",
    "rel_cfd['English'] = fdist_per(rel_fd2)\n",
    "\n",
    "\n",
    "rel_cfd.plot(cumulative=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
